"""
Methods for interacting with 'timegrid' style HDF5s, which each cover 1/6 of
CONUS over a 3 month period, and store their data as a (T,P,Q,F) dynamic grid
with (P,Q,F) static grids and (T,1) timestamps
"""
import numpy as np
import pickle as pkl
import random as rand
import json
import h5py
from datetime import datetime
from pathlib import Path
from multiprocessing import Pool
import matplotlib.pyplot as plt
import cartopy.crs as ccrs
import cartopy.feature as cfeature

from list_feats import nldas_record_mapping,noahlsm_record_mapping

from krttdkit.operate import enhance as enh
from krttdkit.visualize import guitools as gt
from krttdkit.visualize import geoplot as gp
from krttdkit.acquire import grib_tools

def pixelwise_stats(timegrid:Path):
    """
    Calculate monthly min, max, mean, and stdev of each dynamic feature on the
    grid as a series of (P,Q,F,4) arrays of year/month combinations on the PxQ
    grid of F features, each having 4 stats (min, max, mean, stdev).

    :@param timegrid: Path to an hdf5 timegrid generated by extract_feats.py
    :@return: length M list of 2-tuples (year_months, stats) where year_months
        are 2-tuples (year, month) and the corresponding 'stats' is a (P,Q,F,4)
        array as specified above.
    """
    print(f"Opening {timegrid.name}")
    F = h5py.File(timegrid)
    D = F["/data/dynamic"]
    T = [datetime.fromtimestamp(int(t)) for t in tuple(F["/data/time"][...])]
    ## well-ordered tuples of the year/month combination of each timestep
    all_year_months = [(t.year,t.month) for t in T]
    ## Unique year/month combinations
    unq_year_months = tuple(set(all_year_months))
    mins,maxs,means,stdevs = [],[],[],[]
    stats = []
    for unq_ym in unq_year_months:
        ## Extract grids corresponding to this year/month combination
        print(f"Extracting {unq_ym}")
        m_ym = np.array([ym == unq_ym for ym in all_year_months])
        X = D[m_ym,...]
        tmp_stats = np.zeros((*D.shape[1:], 4))
        tmp_stats[...,0] = np.amin(X, axis=0)
        tmp_stats[...,1] = np.amax(X, axis=0)
        tmp_stats[...,2] = np.average(X, axis=0)
        tmp_stats[...,3] = np.std(X, axis=0)
        stats.append(tmp_stats)
    return list(zip(unq_year_months,stats))

def collect_gridstats(gridstat_paths, gridstat_slices,
        new_h5_path, static_pkl_path, feat_labels, chunk_shape=None):
    """
    Quick-and-dirty method to convert regional 'gridstat' files from those
    generated by pixelwise_stats into a single (T,Y,X,F,4) hdf5 grid for
    T months on a Y,X grid having F features described in terms of by 4
    stats (min, max, mean, stdev).

    Note that the hdf5 spatial grid shape is inferred from the static pkl,
    so make sure that the provided static pkl arrays are on the same grid
    as the combined parent domain of the regional gridstat files

    :@param gridstat_paths: List of paths to gridstat hdf5 files
    :@param gridstat_slices: List of 2-tuples containing  slice objects
        corresponding to the y and x position (respectively) of each gridstat
        file's grid on the 2d (Y,X) spatial grid.
    :@param new_h5_path: Path to the hdf5 file that will be generated.
    :@param static_pkl_path: Path to an existing pkl of static values and
        labels (generated by get_static_data) which are stored in a dataset
        alongside the grid stats. The spatial dimensions of the resulting hdf5
        are determined from these grid shapes.
    :@param feat_labels: list of string labels corresponding to the nldas/noah
        features present in the gridstat files (inhereted from timegrids).
    :@param chunk_shape: Shape of hdf5 chunks
    """
    assert len(gridstat_paths)==len(gridstat_slices)
    slabels,sdata = pkl.load(static_pkl_path.open("rb"))
    sdata = np.stack(sdata, axis=-1)

    fg_dict_gridstat = {
            "clabels":("month","lat","lon","feat"),
            "flabels":["min", "max", "mean", "stdev"],
            "meta":{
                "nldas_flabels":feat_labels,
                }
            }
    fg_dict_static = {
            "clabels":("lat","lon"),
            "flabels":tuple(slabels),
            "meta":{}
            }

    F = None
    for p,s in zip(gridstat_paths,gridstat_slices):
        years_months,stats = pkl.load(p.open("rb"))
        if F is None:
            F = h5py.File(
                    name=new_h5_path,
                    mode="w-",
                    rdcc_nbytes=128*1024**2, ## use a 128MB cache
                    )
            G = F.create_group("/data")
            og_years_months = years_months
            fg_dict_gridstat["meta"]["years_months"] = years_months
            G.attrs["gridstats"] = json.dumps(fg_dict_gridstat)
            G.attrs["static"] = json.dumps(fg_dict_static)
            D = G.create_dataset(
                    name="gridstats",
                    shape=(stats.shape[0],*sdata.shape[:2],*stats.shape[-2:]),
                    chunks=chunk_shape,
                    compression="gzip",
                    )
            S = G.create_dataset(name="static", shape=sdata.shape)
            S[...] = sdata
        assert years_months == og_years_months, \
                "regional gridstat time frames must be identical"
        D[:,*s,:,:] = stats
    F.close()

def geo_plot(data, latitude, longitude, bounds=None, plot_spec={},
             show=False, fig_path=None):
    """
    Plot a gridded scalar value on a geodetic domain, using cartopy for borders
    """
    ps = {"xlabel":"", "ylabel":"", "marker_size":4,
          "cmap":"jet_r", "text_size":12, "title":"",
          "norm":None,"figsize":(12,12), "marker":"o", "cbar_shrink":1.,
          "map_linewidth":2}
    plt.clf()
    ps.update(plot_spec)
    plt.rcParams.update({"font.size":ps["text_size"]})

    ax = plt.axes(projection=ccrs.PlateCarree())
    fig = plt.gcf()
    if bounds is None:
        bounds = [np.amin(longitude), np.amax(longitude),
                  np.amin(latitude), np.amax(latitude)]
    ax.set_extent(bounds, crs=ccrs.PlateCarree())

    ax.add_feature(cfeature.LAND, linewidth=ps.get("map_linewidth"))
    #ax.add_feature(cfeature.LAKES, linewidth=ps.get("map_linewidth"))
    #ax.add_feature(cfeature.RIVERS, linewidth=ps.get("map_linewidth"))

    ax.set_title(ps.get("title"))
    ax.set_xlabel(ps.get("xlabel"))
    ax.set_ylabel(ps.get("ylabel"))

    scat = ax.contourf(longitude, latitude, data, cmap=ps.get("cmap"))

    ax.add_feature(cfeature.BORDERS, linewidth=ps.get("map_linewidth"),
                   zorder=120)
    ax.add_feature(cfeature.STATES, linewidth=ps.get("map_linewidth"),
                   zorder=120)
    ax.coastlines()
    fig.colorbar(scat, ax=ax, shrink=ps.get("cbar_shrink"))

    if not fig_path is None:
        fig.set_size_inches(*ps.get("figsize"))
        fig.savefig(fig_path.as_posix(), bbox_inches="tight",dpi=80)
    if show:
        plt.show()

if __name__=="__main__":
    data_dir = Path("data")
    tg_dir = data_dir.joinpath("timegrids")
    static_pkl_path = data_dir.joinpath("static/nldas_static_cropped.pkl")
    gridstat_dir = Path("data/grid_stats")

    '''
    """
    Create regional gridstat pickle files, which aggregate monthly statistics
    (min, max, mean, stdev) for each feature in a timegrid
    """
    #substr = "y000-098_x000-154"
    #substr = "y000-098_x154-308"
    #substr = "y000-098_x308-462"
    #substr = "y098-195_x000-154"
    #substr = "y098-195_x154-308"
    substr = "y098-195_x308-462"
    workers = 4
    timegrids = [p for p in tg_dir.iterdir() if substr in p.name]
    with Pool(workers) as pool:
        results = []
        for r in pool.imap_unordered(pixelwise_stats, timegrids):
            print(f"Finished {[t[0] for t in r]}")
            results += r
    years_months,stats = zip(*list(sorted(results, key=lambda r:r[0])))
    stats = np.stack(stats, axis=0)
    pkl_name = f"gridstats_{'-'.join(map(str,years_months[0]))}_" + \
            f"{'-'.join(map(str,years_months[-1]))}_{substr}.pkl"
    pkl.dump((years_months,stats), gridstat_dir.joinpath(pkl_name).open("wb"))
    '''

    '''
    """ Aggregate regional gridstat pkl files into a single hdf5 """
    gs_paths = [
            p for p in gridstat_dir.iterdir()
            if "gridstats" in p.name and p.suffix==".pkl"]
    ## Parse the slice bounds from the region gridstat file path standard name
    gs_slices = [
            tuple(map(lambda s:slice(*tuple(map(int,s[1:].split("-")))),t))
            for t in [p.stem.split("_")[-2:] for p in gs_paths]]
    ## Assume all labels specified in list_feats are present
    _,nl_labels = map(list,zip(*nldas_record_mapping))
    _,no_labels = map(list,zip(*noahlsm_record_mapping))
    collect_gridstats(
            gridstat_paths=gs_paths,
            gridstat_slices=gs_slices,
            new_h5_path=gridstat_dir.joinpath("full_grid_stats.h5"),
            static_pkl_path=static_pkl_path,
            feat_labels=nl_labels+no_labels,
            chunk_shape=(3,64,64,8,4),
            )

    '''

    '''
    """ Save overall average values as a numpy array"""
    F = h5py.File(gridstat_dir.joinpath("gridstats_full.h5"))
    D = F["/data/gridstats"][...]
    S = F["/data/static"]
    D = np.average(D, axis=0)
    np.save(Path("data/grid_stats/gridstats_avg.npy"), D)
    '''

    slabels,sdata = pkl.load(static_pkl_path.open("rb"))
    _,nl_labels = map(list,zip(*nldas_record_mapping))
    _,no_labels = map(list,zip(*noahlsm_record_mapping))
    flabels = nl_labels+no_labels
    avgs = np.load(Path("data/grid_stats/gridstats_avg.npy"))

    avgs[sdata[slabels.index("m_9999")]] = np.nan

    soilm_labels = ("soilm-10","soilm-40","soilm-100","soilm-200",)
    soilm = avgs[..., tuple(flabels.index(s) for s in soilm_labels), 2]
    tsoil_labels = ("tsoil-10","tsoil-40","tsoil-100","tsoil-200",)
    tsoil = avgs[..., tuple(flabels.index(s) for s in tsoil_labels), 2]
    geo_plot(
            #data=avgs[...,flabels.index("apcp"),2],
            data=avgs[...,flabels.index("veg"),2],
            #data=np.average(tsoil, axis=-1),
            #data=np.sum(soilm, axis=-1),
            latitude=sdata[slabels.index("lat")],
            longitude=sdata[slabels.index("lon")],
            plot_spec={
                #"title":"2012-2022 Mean Full-Column Soil Moisture (kg/m^3)"
                #"title":"2012-2022 Mean Full-Column Soil Temperature (K)"
                #"title":"2012-2022 Mean hourly precipitation (kg/m^2)"
                "title":"Mean vegetation fraction (%)"
                },
            show=True,
            fig_path=None
            )

    exit(0)

    tmp = D[0,:,:,17,:]
    mask = (tmp[...,1] == 9999.)
    tmp[mask] = 0.
    for i in range(tmp.shape[-1]):
        tmp_rgb = gt.scal_to_rgb(tmp[...,i])
        tmp_rgb[mask] = 0
        gp.generate_raw_image(
                (tmp_rgb*255).astype(np.uint8),
                Path(f"figures/tmp_{i}.png")
                )
    #'''

    exit(0)

