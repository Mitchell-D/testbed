"""
Methods for interacting with 'timegrid' style HDF5s, which each cover 1/6 of
CONUS over a 3 month period, and store their data as a (T,P,Q,F) dynamic grid
with (P,Q,F) static grids and (T,1) timestamps
"""
import numpy as np
import pickle as pkl
import random as rand
import json
import h5py
from datetime import datetime
from pathlib import Path
from multiprocessing import Pool
import matplotlib.pyplot as plt
import cartopy.crs as ccrs
import cartopy.feature as cfeature

from list_feats import nldas_record_mapping,noahlsm_record_mapping

from krttdkit.operate import enhance as enh
from krttdkit.visualize import guitools as gt
from krttdkit.visualize import geoplot as gp
from krttdkit.acquire import grib_tools

def parse_timegrid_path(timegrid_path:Path):
    """
    Parse the timegrid file naming scheme

    timegrid naming template:
    timegrid_{YYYY}q{Q}_y{start_y}-{end_y}_x{start_x}-{end_x}.h5

    :@param timegrid_path: Path to a timegrid-style file.
    :@return: Well-ordered 4-tuple like ((year, quarter), (y0,yf), (x0,xf))
        where (year,quarter) describes the time period of the file, and the
        spatial (y0,yf) and (x0,xf) describe the extent of this regional tile
        in the full array (after the cropping from extract_timegrid)
    """
    _,year_quarter,y_range,x_range = timegrid_path.stem.split("_")
    year,quarter = map(int, year_quarter.split("q"))
    y0,yf = map(int, y_range[1:].split("-"))
    x0,xf = map(int, x_range[1:].split("-"))
    return ((year, quarter), (y0,yf), (x0,xf))

def pixelwise_stats(timegrid:Path):
    """
    Calculate monthly min, max, mean, and stdev of each dynamic feature on the
    grid as a series of (P,Q,F,4) arrays of year/month combinations on the PxQ
    grid of F features, each having 4 stats (min, max, mean, stdev).

    :@param timegrid: Path to an hdf5 timegrid generated by extract_timegrid
    :@return: length M list of 2-tuples (year_months, stats) where year_months
        are 2-tuples (year, month) and the corresponding 'stats' is a (P,Q,F,4)
        array as specified above.
    """
    print(f"Opening {timegrid.name}")
    F = h5py.File(timegrid)
    D = F["/data/dynamic"]
    T = [datetime.fromtimestamp(int(t)) for t in tuple(F["/data/time"][...])]
    ## well-ordered tuples of the year/month combination of each timestep
    all_year_months = [(t.year,t.month) for t in T]
    ## Unique year/month combinations
    unq_year_months = tuple(set(all_year_months))
    mins,maxs,means,stdevs = [],[],[],[]
    stats = []
    for unq_ym in unq_year_months:
        ## Extract grids corresponding to this year/month combination
        print(f"Extracting {unq_ym}")
        m_ym = np.array([ym == unq_ym for ym in all_year_months])
        X = D[m_ym,...]
        tmp_stats = np.zeros((*D.shape[1:], 4))
        tmp_stats[...,0] = np.amin(X, axis=0)
        tmp_stats[...,1] = np.amax(X, axis=0)
        tmp_stats[...,2] = np.average(X, axis=0)
        tmp_stats[...,3] = np.std(X, axis=0)
        stats.append(tmp_stats)
    return list(zip(unq_year_months,stats))

def collect_gridstats(gridstat_paths, gridstat_slices,
        new_h5_path, static_pkl_path, feat_labels, chunk_shape=None):
    """
    Quick-and-dirty method to convert regional 'gridstat' files from those
    generated by pixelwise_stats into a single (T,Y,X,F,4) hdf5 grid for
    T months on a Y,X grid having F features described in terms of by 4
    stats (min, max, mean, stdev).

    Note that the hdf5 spatial grid shape is inferred from the static pkl,
    so make sure that the provided static pkl arrays are on the same grid
    as the combined parent domain of the regional gridstat files

    :@param gridstat_paths: List of paths to gridstat hdf5 files
    :@param gridstat_slices: List of 2-tuples containing  slice objects
        corresponding to the y and x position (respectively) of each gridstat
        file's grid on the 2d (Y,X) spatial grid.
    :@param new_h5_path: Path to the hdf5 file that will be generated.
    :@param static_pkl_path: Path to an existing pkl of static values and
        labels (generated by get_static_data) which are stored in a dataset
        alongside the grid stats. The spatial dimensions of the resulting hdf5
        are determined from these grid shapes.
    :@param feat_labels: list of string labels corresponding to the nldas/noah
        features present in the gridstat files (inhereted from timegrids).
    :@param chunk_shape: Shape of hdf5 chunks
    """
    assert len(gridstat_paths)==len(gridstat_slices)
    slabels,sdata = pkl.load(static_pkl_path.open("rb"))
    sdata = np.stack(sdata, axis=-1)

    fg_dict_gridstat = {
            "clabels":("month","lat","lon","feat"),
            "flabels":["min", "max", "mean", "stdev"],
            "meta":{
                "nldas_flabels":feat_labels,
                }
            }
    fg_dict_static = {
            "clabels":("lat","lon"),
            "flabels":tuple(slabels),
            "meta":{}
            }

    F = None
    for p,s in zip(gridstat_paths,gridstat_slices):
        years_months,stats = pkl.load(p.open("rb"))
        if F is None:
            F = h5py.File(
                    name=new_h5_path,
                    mode="w-",
                    rdcc_nbytes=128*1024**2, ## use a 128MB cache
                    )
            G = F.create_group("/data")
            og_years_months = years_months
            fg_dict_gridstat["meta"]["years_months"] = years_months
            G.attrs["gridstats"] = json.dumps(fg_dict_gridstat)
            G.attrs["static"] = json.dumps(fg_dict_static)
            D = G.create_dataset(
                    name="gridstats",
                    shape=(stats.shape[0],*sdata.shape[:2],*stats.shape[-2:]),
                    chunks=chunk_shape,
                    compression="gzip",
                    )
            S = G.create_dataset(name="static", shape=sdata.shape)
            S[...] = sdata
        assert years_months == og_years_months, \
                "regional gridstat time frames must be identical"
        D[:,*s,:,:] = stats
    F.close()

def geo_plot(data, latitude, longitude, bounds=None, plot_spec={},
             show=False, fig_path=None):
    """
    Plot a gridded scalar value on a geodetic domain, using cartopy for borders
    """
    ps = {"xlabel":"", "ylabel":"", "marker_size":4,
          "cmap":"jet_r", "text_size":12, "title":"",
          "norm":None,"figsize":(12,12), "marker":"o", "cbar_shrink":1.,
          "map_linewidth":2}
    plt.clf()
    ps.update(plot_spec)
    plt.rcParams.update({"font.size":ps["text_size"]})

    ax = plt.axes(projection=ccrs.PlateCarree())
    fig = plt.gcf()
    if bounds is None:
        bounds = [np.amin(longitude), np.amax(longitude),
                  np.amin(latitude), np.amax(latitude)]
    ax.set_extent(bounds, crs=ccrs.PlateCarree())

    ax.add_feature(cfeature.LAND, linewidth=ps.get("map_linewidth"))
    #ax.add_feature(cfeature.LAKES, linewidth=ps.get("map_linewidth"))
    #ax.add_feature(cfeature.RIVERS, linewidth=ps.get("map_linewidth"))

    ax.set_title(ps.get("title"))
    ax.set_xlabel(ps.get("xlabel"))
    ax.set_ylabel(ps.get("ylabel"))

    scat = ax.contourf(longitude, latitude, data, cmap=ps.get("cmap"))

    ax.add_feature(cfeature.BORDERS, linewidth=ps.get("map_linewidth"),
                   zorder=120)
    ax.add_feature(cfeature.STATES, linewidth=ps.get("map_linewidth"),
                   zorder=120)
    ax.coastlines()
    fig.colorbar(scat, ax=ax, shrink=ps.get("cbar_shrink"))

    if not fig_path is None:
        fig.set_size_inches(*ps.get("figsize"))
        fig.savefig(fig_path.as_posix(), bbox_inches="tight",dpi=80)
    if show:
        plt.show()

if __name__=="__main__":
    data_dir = Path("data")
    tg_dir = data_dir.joinpath("timegrids")
    static_pkl_path = data_dir.joinpath("static/nldas_static_cropped.pkl")
    gridstat_dir = Path("data/grid_stats")

    '''
    """
    Create regional gridstat pickle files, which aggregate monthly statistics
    (min, max, mean, stdev) for each feature in a timegrid
    """
    #substr = "y000-098_x000-154"
    #substr = "y000-098_x154-308"
    #substr = "y000-098_x308-462"
    #substr = "y098-195_x000-154"
    #substr = "y098-195_x154-308"
    substr = "y098-195_x308-462"
    workers = 4
    timegrids = [p for p in tg_dir.iterdir() if substr in p.name]
    with Pool(workers) as pool:
        results = []
        for r in pool.imap_unordered(pixelwise_stats, timegrids):
            print(f"Finished {[t[0] for t in r]}")
            results += r
    years_months,stats = zip(*list(sorted(results, key=lambda r:r[0])))
    stats = np.stack(stats, axis=0)
    pkl_name = f"gridstats_{'-'.join(map(str,years_months[0]))}_" + \
            f"{'-'.join(map(str,years_months[-1]))}_{substr}.pkl"
    pkl.dump((years_months,stats), gridstat_dir.joinpath(pkl_name).open("wb"))
    '''

    '''
    """ Aggregate regional gridstat pkl files into a single hdf5 """
    gs_paths = [
            p for p in gridstat_dir.iterdir()
            if "gridstats" in p.name and p.suffix==".pkl"]
    ## Parse the slice bounds from the region gridstat file path standard name
    gs_slices = [
            tuple(map(lambda s:slice(*tuple(map(int,s[1:].split("-")))),t))
            for t in [p.stem.split("_")[-2:] for p in gs_paths]]
    ## Assume all labels specified in list_feats are present
    _,nl_labels = map(list,zip(*nldas_record_mapping))
    _,no_labels = map(list,zip(*noahlsm_record_mapping))
    collect_gridstats(
            gridstat_paths=gs_paths,
            gridstat_slices=gs_slices,
            new_h5_path=gridstat_dir.joinpath("full_grid_stats.h5"),
            static_pkl_path=static_pkl_path,
            feat_labels=nl_labels+no_labels,
            chunk_shape=(3,64,64,8,4),
            )

    #'''

    #'''
    """ Save overall average values as a numpy array"""
    F = h5py.File(gridstat_dir.joinpath("gridstats_full.h5"))
    D = F["/data/gridstats"][...]
    S = F["/data/static"]
    D = np.average(D, axis=0)
    np.save(Path("data/grid_stats/gridstats_avg.npy"), D)
    #'''

    exit(0)

    '''
    """ Plot gridded statistics on a CONUS map """
    slabels,sdata = pkl.load(static_pkl_path.open("rb"))
    _,nl_labels = map(list,zip(*nldas_record_mapping))
    _,no_labels = map(list,zip(*noahlsm_record_mapping))
    flabels = nl_labels+no_labels
    avgs = np.load(Path("data/grid_stats/gridstats_avg.npy"))

    avgs[sdata[slabels.index("m_9999")]] = np.nan

    soilm_labels = ("soilm-10","soilm-40","soilm-100","soilm-200",)
    soilm = avgs[..., tuple(flabels.index(s) for s in soilm_labels), 2]
    tsoil_labels = ("tsoil-10","tsoil-40","tsoil-100","tsoil-200",)
    tsoil = avgs[..., tuple(flabels.index(s) for s in tsoil_labels), 2]
    geo_plot(
            #data=avgs[...,flabels.index("apcp"),2],
            data=avgs[...,flabels.index("veg"),2],
            #data=np.average(tsoil, axis=-1),
            #data=np.sum(soilm, axis=-1),
            latitude=sdata[slabels.index("lat")],
            longitude=sdata[slabels.index("lon")],
            plot_spec={
                #"title":"2012-2022 Mean Full-Column Soil Moisture (kg/m^3)"
                #"title":"2012-2022 Mean Full-Column Soil Temperature (K)"
                #"title":"2012-2022 Mean hourly precipitation (kg/m^2)"
                "title":"Mean vegetation fraction (%)"
                },
            show=True,
            fig_path=None
            )
    #'''

    #'''
    """
    Load a gridstat full-domain average file and reduce its data to a (F_d, 4)
    array of mean valid pixel stats (min, max, mean, stdev) for each feature.

    Use this section to update the normalization coefficients in list_feats
    """
    """ Generate pixel masks for each veg/soil class combination """
    ## Load the full-CONUS static pixel grid
    slabels,sdata = pkl.load(static_pkl_path.open("rb"))
    ## Get the integer-identified soil texture and vegetation class grids
    int_veg = sdata[slabels.index("int_veg")]
    int_soil = sdata[slabels.index("int_soil")]
    m_valid = sdata[slabels.index("m_valid")].astype(bool)

    ## (P,Q,F_d,4) array of statistics for dynamic feats F_d on the (P,Q) grid.
    ## The final dimension indexes the (min, max, mean, stdev) of each feature.
    gridstats = np.load(gridstat_dir.joinpath("gridstats_avg.npy"))
    ## Calculate full-domain averages of all dynamic feature statistics
    gmean,gstdev = map(np.squeeze,np.split(np.mean(
        gridstats[m_valid,:,2:], axis=0), 2, axis=-1))
    gmin = np.amin(gridstats[m_valid,:,0], axis=0)
    gmax = np.amax(gridstats[m_valid,:,1], axis=0)
    _,gslabels = map(tuple,zip(*nldas_record_mapping, *noahlsm_record_mapping))

    for f in set((*window_feats, *horizon_feats, *pred_feats)):
        tmp_idx = gslabels.index(f)
        tmp_min = gmin[tmp_idx]
        tmp_max = gmax[tmp_idx]
        tmp_mean = gmean[tmp_idx]
        tmp_stdev = gstdev[tmp_idx]
        print(f"('{f}', ({tmp_min}, {tmp_max}, {tmp_mean}, {tmp_stdev})),")
    #'''

    '''
    """ Generate basic scalar RGBs of particular features """
    tmp = D[0,:,:,17,:]
    mask = (tmp[...,1] == 9999.)
    tmp[mask] = 0.
    for i in range(tmp.shape[-1]):
        tmp_rgb = gt.scal_to_rgb(tmp[...,i])
        tmp_rgb[mask] = 0
        gp.generate_raw_image(
                (tmp_rgb*255).astype(np.uint8),
                Path(f"figures/tmp_{i}.png")
                )
    '''

    #'''
    """
    Extract ranges describing the timegrids from their file names, and sort
    them by time range, y domain position, then x domain position
    """
    timegrid_paths = tuple(tg_dir.iterdir())
    timegrid_info,timegrid_paths = tuple(zip(*sorted(zip(
        tuple(map(parse_timegrid_path, timegrid_paths)),
        timegrid_paths
        ))))
    #'''

