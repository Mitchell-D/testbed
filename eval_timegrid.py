"""
Methods for interacting with 'timegrid' style HDF5s, which each cover 1/6 of
CONUS over a 3 month period, and store their data as a (T,P,Q,F) dynamic grid
with (P,Q,F) static grids and (T,1) timestamps
"""
import numpy as np
import pickle as pkl
import random as rand
import json
import h5py
from datetime import datetime
from pathlib import Path
from multiprocessing import Pool
import matplotlib.pyplot as plt
import cartopy.crs as ccrs
import cartopy.feature as cfeature

from list_feats import nldas_record_mapping,noahlsm_record_mapping

from krttdkit.operate import enhance as enh
from krttdkit.visualize import guitools as gt
from krttdkit.visualize import geoplot as gp
from krttdkit.acquire import grib_tools

def parse_timegrid_path(timegrid_path:Path):
    """
    Parse the timegrid file naming scheme

    timegrid naming template:
    timegrid_{YYYY}q{Q}_y{start_y}-{end_y}_x{start_x}-{end_x}.h5

    :@param timegrid_path: Path to a timegrid-style file.
    :@return: Well-ordered 4-tuple like ((year, quarter), (y0,yf), (x0,xf))
        where (year,quarter) describes the time period of the file, and the
        spatial (y0,yf) and (x0,xf) describe the extent of this regional tile
        in the full array (after the cropping from extract_timegrid)
    """
    _,year_quarter,y_range,x_range = timegrid_path.stem.split("_")
    year,quarter = map(int, year_quarter.split("q"))
    y0,yf = map(int, y_range[1:].split("-"))
    x0,xf = map(int, x_range[1:].split("-"))
    return ((year, quarter), (y0,yf), (x0,xf))

def pixelwise_stats(timegrid:Path):
    """
    Calculate monthly min, max, mean, and stdev of each dynamic feature on the
    grid as a series of (P,Q,F,4) arrays of year/month combinations on the PxQ
    grid of F features, each having 4 stats (min, max, mean, stdev).

    :@param timegrid: Path to an hdf5 timegrid generated by extract_timegrid
    :@return: length M list of 2-tuples (year_months, stats) where year_months
        are 2-tuples (year, month) and the corresponding 'stats' is a (P,Q,F,4)
        array as specified above.
    """
    print(f"Opening {timegrid.name}")
    F = h5py.File(timegrid)
    D = F["/data/dynamic"]
    T = [datetime.fromtimestamp(int(t)) for t in tuple(F["/data/time"][...])]
    ## well-ordered tuples of the year/month combination of each timestep
    all_year_months = [(t.year,t.month) for t in T]
    ## Unique year/month combinations
    unq_year_months = tuple(set(all_year_months))
    mins,maxs,means,stdevs = [],[],[],[]
    stats = []
    for unq_ym in unq_year_months:
        ## Extract grids corresponding to this year/month combination
        print(f"Extracting {unq_ym}")
        m_ym = np.array([ym == unq_ym for ym in all_year_months])
        X = D[m_ym,...]
        tmp_stats = np.zeros((*D.shape[1:], 4))
        tmp_stats[...,0] = np.amin(X, axis=0)
        tmp_stats[...,1] = np.amax(X, axis=0)
        tmp_stats[...,2] = np.average(X, axis=0)
        tmp_stats[...,3] = np.std(X, axis=0)
        stats.append(tmp_stats)
    return list(zip(unq_year_months,stats))

def make_gridstat_hdf5(timegrids:list, out_file:Path, derived_feats:dict=None,
        debug=False):
    """
    Calculate pixel-wise monthly min, max, mean, and standard deviation of each
    stored dynamic and derived feature in the timegrids and store the
    statistics alongside static data in a new hdf5 file.
    """
    ## Collect labels and static data from the timegrids
    tg_shape,tg_dlabels,tg_slabels,tg_static,m_valid = None,None,None,None,None
    tgs_months = []
    ## verify that all provided timegrids are on the same grid and have
    ## uniform features and valid pixel masks.
    for tg in timegrids:
        ## 128MB cache with 256 slots; each chunk is a little over 1/3 MB
        tg_open = h5py.File(tg, "r", rdcc_nbytes=128*1024**2, rdcc_nslots=256)
        if tg_shape is None:
            tg_shape = tg_open["/data/dynamic"].shape
            ## collect dynamic and static feature labels
            tg_dlabels = json.loads(
                    tg_open["data"].attrs["dynamic"])["flabels"]
            tg_slabels = json.loads(
                    tg_open["data"].attrs["static"])["flabels"]
            tg_static = tg_open["/data/static"][...]
            m_valid = tg_static[...,tg_slabels.index("m_valid")]
        else:
            assert tg_shape[1:] == tg_open["/data/dynamic"].shape[1:], \
                    "Timegrid grid shapes & feature count must be uniform"
            tmp_vld = tg_open["/data/static"][...,tg_slabels.index("m_valid")]
            assert np.all(m_valid == tmp_vld)

        tmp_months = np.array([
                datetime.fromtimestamp(int(t)).month
                for t in tuple(tg_open["/data/time"][...])
                ], dtype=np.uint8)
        ## keep the open timegrid file alongside its months mask so that its
        ## buffer persists during iteration over features
        tgs_months.append((tg_open, tmp_months))

    ## convert to a boolean mask
    m_valid = (m_valid == 1.)
    ## collect all feature labels, whether stored  or derived
    all_flabels = tg_dlabels + list(derived_feats.keys())
    F = h5py.File(name=out_file.as_posix(), mode="w-", rdcc_nbytes=256*1024**2)
    ## stats shape for 12 months on (P,Q,F) grid with 4 stats per feature
    stats_shape = (12, *tg_shape[1:3], len(all_flabels), 4)
    ## create hdf5 datasets for gridstats, static data, and histograms
    G = F.create_dataset(
            name="/data/gridstats",
            shape=stats_shape,
            maxshape=stats_shape,
            chunks=(12,32,32,8,4),
            compression="gzip",
            )
    S = F.create_dataset(name="/data/static", shape=tg_static.shape)
    S[...] = tg_static
    F["data"].attrs["dlabels"] = json.dumps(all_flabels)
    F["data"].attrs["slabels"] = json.dumps(tg_slabels)

    print("starting to extract months...")
    for fidx,flabel in enumerate(all_flabels):
        if debug:
            print(f"Extracting {flabel}")
        tmp_stats = np.zeros((*stats_shape[:3], 4))
        ## Collect monthly data from all timegrids for each feature.
        ## monthly sub-arrays are (hours,pixels) shaped for each px in m_valid
        month_arrays = [[] for j in range(12)]
        for tgo,m_months in tgs_months:
            for m in np.unique(m_months):
                m_match = (m_months==m)
                if flabel in tg_dlabels:
                    ix = tg_dlabels.index(flabel)
                    tmp_subarr = tgo["/data/dynamic"][m_match,...,ix]
                    tmp_subarr = tmp_subarr[:,m_valid]
                ## if not a stored feature, key must be a derived feature
                else:
                    ## extract arguments for and evaluate derived features
                    sd_labels,ss_labels,fun = derived_feats[flabel]
                    sd_idxs = tuple(tg_dlabels.index(k) for k in sd_labels)
                    ss_idxs = tuple(tg_slabels.index(k) for k in ss_labels)
                    tmp_subarr = tgo["/data/dynamic"][m_match,...][:,m_valid]
                    sd_args = tuple(tmp_subarr[...,j] for j in sd_idxs)
                    tmp_static = tg_static[m_valid]
                    ss_args = tuple(tmp_static[...,j] for j in ss_idxs)
                    tmp_subarr = eval(fun)(sd_args, ss_args)
                if debug:
                    print(f"{m} {tmp_subarr.shape = }")
                month_arrays[m-1].append(tmp_subarr[:,:,])

        for m,ma in enumerate(month_arrays, 1):
            ma = np.concatenate(ma, axis=0)
            tmp_stats[m-1,m_valid,:] = np.stack((
                np.amin(ma, axis=0),
                np.amax(ma, axis=0),
                np.average(ma, axis=0),
                np.std(ma, axis=0),
                ), axis=-1)
        if debug:
            print(f"Loading {flabel}")
            print(tmp_stats.shape)
        G[:,:,:,fidx,:] = tmp_stats
    for tgo,_ in tgs_months:
        tgo.close()
    return

def collect_gridstats(gridstat_paths, gridstat_slices,
        new_h5_path, static_pkl_path, feat_labels, chunk_shape=None):
    """
    Quick-and-dirty method to convert regional 'gridstat' files from those
    generated by pixelwise_stats into a single (T,Y,X,F,4) hdf5 grid for
    T months on a Y,X grid having F features described in terms of by 4
    stats (min, max, mean, stdev).

    Note that the hdf5 spatial grid shape is inferred from the static pkl,
    so make sure that the provided static pkl arrays are on the same grid
    as the combined parent domain of the regional gridstat files

    :@param gridstat_paths: List of paths to gridstat hdf5 files
    :@param gridstat_slices: List of 2-tuples containing  slice objects
        corresponding to the y and x position (respectively) of each gridstat
        file's grid on the 2d (Y,X) spatial grid.
    :@param new_h5_path: Path to the hdf5 file that will be generated.
    :@param static_pkl_path: Path to an existing pkl of static values and
        labels (generated by get_static_data) which are stored in a dataset
        alongside the grid stats. The spatial dimensions of the resulting hdf5
        are determined from these grid shapes.
    :@param feat_labels: list of string labels corresponding to the nldas/noah
        features present in the gridstat files (inhereted from timegrids).
    :@param chunk_shape: Shape of hdf5 chunks
    """
    assert len(gridstat_paths)==len(gridstat_slices)
    slabels,sdata = pkl.load(static_pkl_path.open("rb"))
    sdata = np.stack(sdata, axis=-1)

    fg_dict_gridstat = {
            "clabels":("month","lat","lon","feat"),
            "flabels":["min", "max", "mean", "stdev"],
            "meta":{
                "nldas_flabels":feat_labels,
                }
            }
    fg_dict_static = {
            "clabels":("lat","lon"),
            "flabels":tuple(slabels),
            "meta":{}
            }

    F = None
    for p,s in zip(gridstat_paths,gridstat_slices):
        years_months,stats = pkl.load(p.open("rb"))
        if F is None:
            F = h5py.File(
                    name=new_h5_path,
                    mode="w-",
                    rdcc_nbytes=128*1024**2, ## use a 128MB cache
                    )
            G = F.create_group("/data")
            og_years_months = years_months
            fg_dict_gridstat["meta"]["years_months"] = years_months
            G.attrs["gridstats"] = json.dumps(fg_dict_gridstat)
            G.attrs["static"] = json.dumps(fg_dict_static)
            D = G.create_dataset(
                    name="gridstats",
                    shape=(stats.shape[0],*sdata.shape[:2],*stats.shape[-2:]),
                    chunks=chunk_shape,
                    compression="gzip",
                    )
            S = G.create_dataset(name="static", shape=sdata.shape)
            S[...] = sdata
        assert years_months == og_years_months, \
                "regional gridstat time frames must be identical"
        D[:,*s,:,:] = stats
    F.close()

def geo_plot(data, latitude, longitude, bounds=None, plot_spec={},
             show=False, fig_path=None):
    """
    Plot a gridded scalar value on a geodetic domain, using cartopy for borders
    """
    ps = {"xlabel":"", "ylabel":"", "marker_size":4,
          "cmap":"jet_r", "text_size":12, "title":"",
          "norm":None,"figsize":(12,12), "marker":"o", "cbar_shrink":1.,
          "map_linewidth":2}
    plt.clf()
    ps.update(plot_spec)
    plt.rcParams.update({"font.size":ps["text_size"]})

    ax = plt.axes(projection=ccrs.PlateCarree())
    fig = plt.gcf()
    if bounds is None:
        bounds = [np.amin(longitude), np.amax(longitude),
                  np.amin(latitude), np.amax(latitude)]
    ax.set_extent(bounds, crs=ccrs.PlateCarree())

    ax.add_feature(cfeature.LAND, linewidth=ps.get("map_linewidth"))
    #ax.add_feature(cfeature.LAKES, linewidth=ps.get("map_linewidth"))
    #ax.add_feature(cfeature.RIVERS, linewidth=ps.get("map_linewidth"))

    ax.set_title(ps.get("title"))
    ax.set_xlabel(ps.get("xlabel"))
    ax.set_ylabel(ps.get("ylabel"))

    scat = ax.contourf(longitude, latitude, data, cmap=ps.get("cmap"))

    ax.add_feature(cfeature.BORDERS, linewidth=ps.get("map_linewidth"),
                   zorder=120)
    ax.add_feature(cfeature.STATES, linewidth=ps.get("map_linewidth"),
                   zorder=120)
    ax.coastlines()
    fig.colorbar(scat, ax=ax, shrink=ps.get("cbar_shrink"))

    if not fig_path is None:
        fig.set_size_inches(*ps.get("figsize"))
        fig.savefig(fig_path.as_posix(), bbox_inches="tight",dpi=80)
    if show:
        plt.show()

if __name__=="__main__":
    data_dir = Path("data")
    tg_dir = data_dir.joinpath("timegrids")
    static_pkl_path = data_dir.joinpath("static/nldas_static_cropped.pkl")
    gridstat_dir = Path("data/grid_stats")

    #'''
    """ Create regional gridstat hdf5 files """
    from list_feats import derived_feats
    #substr = "y000-098_x000-154" ## NW
    #substr = "y000-098_x154-308" ## NC
    #substr = "y000-098_x308-462" ## NE
    #substr = "y098-195_x000-154" ## SW
    #substr = "y098-195_x154-308" ## SC
    substr = "y098-195_x308-462" ## SE

    timegrids = [p for p in tg_dir.iterdir() if substr in p.name]
    print(timegrids)
    make_gridstat_hdf5(
            timegrids=timegrids,
            out_file=gridstat_dir.joinpath(
                f"gridstats_2012-1_2023-12_{substr}.h5"),
            derived_feats=derived_feats,
            debug=True,
            )
    exit(0)
    '''
    workers = 4
    with Pool(workers) as pool:
        results = []
        for r in pool.imap_unordered(pixelwise_stats, timegrids):
            print(f"Finished {[t[0] for t in r]}")
            results += r
    '''


    '''
    """
    Create regional gridstat pickle files, which aggregate monthly statistics
    (min, max, mean, stdev) for each feature in a timegrid by
    """
    #substr = "y000-098_x000-154"
    #substr = "y000-098_x154-308"
    #substr = "y000-098_x308-462"
    #substr = "y098-195_x000-154"
    #substr = "y098-195_x154-308"
    substr = "y098-195_x308-462"
    workers = 4
    timegrids = [p for p in tg_dir.iterdir() if substr in p.name]
    with Pool(workers) as pool:
        results = []
        for r in pool.imap_unordered(pixelwise_stats, timegrids):
            print(f"Finished {[t[0] for t in r]}")
            results += r
    years_months,stats = zip(*list(sorted(results, key=lambda r:r[0])))
    stats = np.stack(stats, axis=0)
    pkl_name = f"gridstats_{'-'.join(map(str,years_months[0]))}_" + \
            f"{'-'.join(map(str,years_months[-1]))}_{substr}.pkl"
    pkl.dump((years_months,stats), gridstat_dir.joinpath(pkl_name).open("wb"))
    '''

    '''
    """ Aggregate regional gridstat pkl files into a single hdf5 """
    gs_paths = [
            p for p in gridstat_dir.iterdir()
            if "gridstats" in p.name and p.suffix==".pkl"]
    ## Parse the slice bounds from the region gridstat file path standard name
    gs_slices = [
            tuple(map(lambda s:slice(*tuple(map(int,s[1:].split("-")))),t))
            for t in [p.stem.split("_")[-2:] for p in gs_paths]]
    ## Assume all labels specified in list_feats are present
    _,nl_labels = map(list,zip(*nldas_record_mapping))
    _,no_labels = map(list,zip(*noahlsm_record_mapping))
    collect_gridstats(
            gridstat_paths=gs_paths,
            gridstat_slices=gs_slices,
            new_h5_path=gridstat_dir.joinpath("full_grid_stats.h5"),
            static_pkl_path=static_pkl_path,
            feat_labels=nl_labels+no_labels,
            chunk_shape=(3,64,64,8,4),
            )

    '''

    #'''
    """ Save overall average values as a numpy array"""
    F = h5py.File(gridstat_dir.joinpath("gridstats_full.h5"))
    D = F["/data/gridstats"][...]
    S = F["/data/static"]
    D = np.average(D, axis=0)
    np.save(Path("data/grid_stats/gridstats_avg.npy"), D)
    #'''

    exit(0)

    '''
    """ Plot gridded statistics on a CONUS map """
    slabels,sdata = pkl.load(static_pkl_path.open("rb"))
    _,nl_labels = map(list,zip(*nldas_record_mapping))
    _,no_labels = map(list,zip(*noahlsm_record_mapping))
    flabels = nl_labels+no_labels
    avgs = np.load(Path("data/grid_stats/gridstats_avg.npy"))

    avgs[sdata[slabels.index("m_9999")]] = np.nan

    soilm_labels = ("soilm-10","soilm-40","soilm-100","soilm-200",)
    soilm = avgs[..., tuple(flabels.index(s) for s in soilm_labels), 2]
    tsoil_labels = ("tsoil-10","tsoil-40","tsoil-100","tsoil-200",)
    tsoil = avgs[..., tuple(flabels.index(s) for s in tsoil_labels), 2]
    geo_plot(
            #data=avgs[...,flabels.index("apcp"),2],
            data=avgs[...,flabels.index("veg"),2],
            #data=np.average(tsoil, axis=-1),
            #data=np.sum(soilm, axis=-1),
            latitude=sdata[slabels.index("lat")],
            longitude=sdata[slabels.index("lon")],
            plot_spec={
                #"title":"2012-2022 Mean Full-Column Soil Moisture (kg/m^3)"
                #"title":"2012-2022 Mean Full-Column Soil Temperature (K)"
                #"title":"2012-2022 Mean hourly precipitation (kg/m^2)"
                "title":"Mean vegetation fraction (%)"
                },
            show=True,
            fig_path=None
            )
    #'''

    #'''
    """
    Load a gridstat full-domain average file and reduce its data to a (F_d, 4)
    array of mean valid pixel stats (min, max, mean, stdev) for each feature.

    Use this section to update the normalization coefficients in list_feats
    """
    """ Generate pixel masks for each veg/soil class combination """
    ## Load the full-CONUS static pixel grid
    slabels,sdata = pkl.load(static_pkl_path.open("rb"))
    ## Get the integer-identified soil texture and vegetation class grids
    int_veg = sdata[slabels.index("int_veg")]
    int_soil = sdata[slabels.index("int_soil")]
    m_valid = sdata[slabels.index("m_valid")].astype(bool)

    ## (P,Q,F_d,4) array of statistics for dynamic feats F_d on the (P,Q) grid.
    ## The final dimension indexes the (min, max, mean, stdev) of each feature.
    gridstats = np.load(gridstat_dir.joinpath("gridstats_avg.npy"))
    ## Calculate full-domain averages of all dynamic feature statistics
    gmean,gstdev = map(np.squeeze,np.split(np.mean(
        gridstats[m_valid,:,2:], axis=0), 2, axis=-1))
    gmin = np.amin(gridstats[m_valid,:,0], axis=0)
    gmax = np.amax(gridstats[m_valid,:,1], axis=0)
    _,gslabels = map(tuple,zip(*nldas_record_mapping, *noahlsm_record_mapping))

    for f in set((*window_feats, *horizon_feats, *pred_feats)):
        tmp_idx = gslabels.index(f)
        tmp_min = gmin[tmp_idx]
        tmp_max = gmax[tmp_idx]
        tmp_mean = gmean[tmp_idx]
        tmp_stdev = gstdev[tmp_idx]
        print(f"('{f}', ({tmp_min}, {tmp_max}, {tmp_mean}, {tmp_stdev})),")
    #'''

    '''
    """ Generate basic scalar RGBs of particular features """
    tmp = D[0,:,:,17,:]
    mask = (tmp[...,1] == 9999.)
    tmp[mask] = 0.
    for i in range(tmp.shape[-1]):
        tmp_rgb = gt.scal_to_rgb(tmp[...,i])
        tmp_rgb[mask] = 0
        gp.generate_raw_image(
                (tmp_rgb*255).astype(np.uint8),
                Path(f"figures/tmp_{i}.png")
                )
    '''

    #'''
    """
    Extract ranges describing the timegrids from their file names, and sort
    them by time range, y domain position, then x domain position
    """
    timegrid_paths = tuple(tg_dir.iterdir())
    timegrid_info,timegrid_paths = tuple(zip(*sorted(zip(
        tuple(map(parse_timegrid_path, timegrid_paths)),
        timegrid_paths
        ))))
    #'''

