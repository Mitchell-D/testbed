import pickle as pkl
from pathlib import Path
import numpy as np
from datetime import datetime
from datetime import timedelta

# It's temporary thanks to warnings from the conda build of tensorflow I need
import warnings
warnings.filterwarnings("ignore")

from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import InputLayer, LSTM, Dense, Bidirectional
from tensorflow.keras.layers import Dropout, Concatenate
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.losses import MeanSquaredError
from tensorflow.keras.metrics import RootMeanSquaredError
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import L2
from tensorflow.keras import Input, Model

def lstm_static_bidir(
        window_size, feature_dims, static_dims, rec_reg_penalty=0.01,
        stat_reg_penalty=0.001, drop_rate=0.1):
    """
    :@return: Uncompiled Model object
    """
    r_input = Input(shape=(window_size, feature_dims), name="rec_in")
    s_input = Input(shape=(static_dims,), name="stat_in")

    # First bidirectional LSTM layer: output dims = 128 with dropout layer
    r_lstm_1 = LSTM(
            units=128,
            kernel_regularizer=L2(rec_reg_penalty),
            recurrent_regularizer=L2(rec_reg_penalty),
            return_sequences=True,
            )
    r_bd_1 = Bidirectional(r_lstm_1, name="bd_1")(r_input)
    r_1 = Dropout(drop_rate, name="drop_1")(r_bd_1)

    # Second bidirectional LSTM layer: output dims = 64
    r_lstm_2 = LSTM(
            units=64,
            kernel_regularizer=L2(rec_reg_penalty),
            recurrent_regularizer=L2(rec_reg_penalty)
            )
    r_bd_2 = Bidirectional(r_lstm_2, name="bd_2")(r_1)
    r_2 = Dropout(drop_rate, name="drop_2")(r_bd_2)

    # Dense static layer: output dims = 64
    s_1 = Dense(
            units=64,
            kernel_regularizer=L2(stat_reg_penalty),
            activation="relu",
            name="s_dense")(s_input)

    # Concatenation layer + 2 dense layers
    concat = Concatenate(axis=1, name="rs_concat")([r_2, s_1])
    combo_dense = Dense(units=64, activation="relu", name="combo")(concat)
    output = Dense(units=1, activation="sigmoid", name="output")(combo_dense)

    model = Model(inputs=[r_input, s_input], outputs=[output])
    return model


def basic_deep_lstm(window_size:int, feature_dims:int, output_dims:int):
    """
    -> batch shape: (batch_size, window_size, feature_dims)
    -> output shape: (batch_size, output_dims)

    batch shape:
    Batch size is (timesteps - window_size) since the first "window"
    of features must be used to inform the next time step in training.

    :@param 1d_data_dict: Dictionary of 1D data following the standard of
        dictionaries generated by make_1d_dataset.py
    :@param window_size: Number of former timesteps of input features
        that are trained on for each additional prediction. To my
        understanding, periodic features with a frequency less than the time
        window won't be fully characterized unless the LSTM is stateful.
    """
    nldas1D = Sequential()

    # First layer's output shape is (batch_size, window_size, hidden_size)
    # since return_sequences is True. This returns a 3D tensor containing an
    # abstract time series sequence for the next LSTM layer to process.
    nldas1D.add(LSTM(
        units=64,
        input_shape=(window_size, feature_dims),
        # Return sequences of input
        return_sequences=True,
        activation="relu",
        ))
    # return_sequences set to False here to collapse a dimension
    nldas1D.add(LSTM(units=32, return_sequences=False))
    nldas1D.add(Dense(units=8, activation='relu'))
    nldas1D.add(Dense(units=output_dims, activation='linear'))
    nldas1D.compile(optimizer="adam", loss="mse")

    nldas1D.summary()
    return nldas1D

if __name__=="__main__":
    # First cycle only
    #training_pkl = Path("data/model_data/silty-loam_set1_training.pkl")
    #validation_pkl = Path("data/model_data/silty-loam_set1_validation.pkl")
    model_dir = Path("models/set004")

    # All cycles
    t_pkl = model_dir.joinpath("input/silty-loam_set4_training.pkl")
    v_pkl = model_dir.joinpath("input/silty-loam_set4_validation.pkl")
    s_pkl = model_dir.joinpath("input/silty-loam_set4_testing.pkl")

    #checkpoint_file = Path("data/model_check/set001")
    checkpoint_file = model_dir.joinpath("checkpoint")

    t_feat,t_truth,t_times = pkl.load(t_pkl.open("rb"))
    v_feat,v_truth,v_times = pkl.load(v_pkl.open("rb"))
    s_feat,s_truth,s_times = pkl.load(s_pkl.open("rb"))

    #'''
    # set1: 5 epochs, first cycle
    # set2: 200 epochs, first cycle
    # set3: 600 epochs, all 4 cycles
    EPOCHS = 600
    model = basic_deep_lstm(
            window_size=48,
            feature_dims=t_feat.shape[2],
            output_dims=t_truth.shape[1],
            )
    check = ModelCheckpoint(checkpoint_file.as_posix(), save_best_only=True)
    model.compile(
            loss=MeanSquaredError(),
            optimizer=Adam(learning_rate=1e-4),
            metrics=[RootMeanSquaredError()],
            )
    model.fit(
            t_feat,
            t_truth,
            validation_data=(v_feat, v_truth),
            #epochs=30,
            epochs=EPOCHS,
            callbacks=[check],
            )
    #'''

    """
    Re-load the model and generate predictions for training, validation, and
    testing data.
    """
    print(f"Loading {checkpoint_file.as_posix()}")
    model = load_model(checkpoint_file.as_posix())
    model.compile(optimizer='adam')

    t_out = model.predict(t_feat)
    v_out = model.predict(v_feat)
    s_out = model.predict(s_feat)
    print("features:",t_feat.shape,v_feat.shape,s_feat.shape)
    print("outputs:",t_out.shape, v_out.shape, s_out.shape)
    pkl.dump((t_out, v_out, s_out),
             model_dir.joinpath("output/silty-loam_set003_out.pkl").open("wb"))
