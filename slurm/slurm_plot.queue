#!/bin/csh
### SLURM batch script

### Email address
#SBATCH --mail-user=mtd0012@uah.edu


### TOTAL processors (number of tasks)
###SBATCH --ntasks 11
#SBATCH --ntasks 2

### total run time estimate (D-HH:MM)
#SBATCH -t 3-00:00

### memory (MB per CPU)
#SBATCH --mem-per-cpu=16G

### Mail to user on job done and fail
#SBATCH --mail-type=END,FAIL

### Partition (queue), select shared for GPU
### Optionally specify a GPU type: --gres=gpu:rtx5000:1 or --gres=gpu:a100:1
###SBATCH -p shared --gres=gpu:a100:1
###SBATCH -p shared --gres=gpu:rtx5000:1
###SBATCH -p shared --gres=gpu:1
#SBATCH -p standard

#SBATCH -J grid_plot
###SBATCH --open-mode=append ### Don't overwrite existing files
#SBATCH -o /rhome/mdodson/testbed/slurm/out/slurm_plot.out ## STDOUT
#SBATCH -e /rhome/mdodson/testbed/slurm/out/slurm_plot.err ## STDERR

cd /rhome/mdodson/testbed/scripts

set runcmd = /rhome/mdodson/.micromamba/envs/learn3/bin/python

#${runcmd} -u plot_function_line.py
${runcmd} -u plot_gridstats.py
#${runcmd} -u plot_grid_pred_ensembles.py
#${runcmd} -u plot_grid_pred_bulk.py
#${runcmd} -u plot_grid_eval.py
#${runcmd} -u plot_grid_pred_eval.py
#${runcmd} -u plot_performance_pkls.py
#${runcmd} -u plot_sequence_pred_samples.py
#${runcmd} -u plot_sequence_eval.py
