#!/bin/csh
### SLURM batch script

### Email address
#SBATCH --mail-user=mtd0012@uah.edu

### TOTAL processors (number of tasks)
###SBATCH --ntasks 12
#SBATCH --ntasks 3

### total run time estimate (D-HH:MM)
#SBATCH -t 3-00:00

### memory (MB per CPU)
###SBATCH --mem-per-cpu=8G
#SBATCH --mem-per-cpu=16G

### Mail to user on job done and fail
#SBATCH --mail-type=END,FAIL

### Partition (queue), select shared for GPU
### Optionally specify a GPU type: --gres=gpu:rtx5000:1 or --gres=gpu:a100:1
###SBATCH -p shared --gres=gpu:a100:1
####SBATCH -p shared --gres=gpu:rtx5000:1
#SBATCH -p shared --gres=gpu:1
###SBATCH -p standard

#SBATCH -J acc-4_grid
###SBATCH --open-mode=append ### Don't overwrite existing files
###SBATCH -o /rhome/mdodson/testbed/slurm/out/slurm_grids_bulk-rsm-9.out ## STDOUT
###SBATCH -e /rhome/mdodson/testbed/slurm/out/slurm_grids_bulk-rsm-9.err ## STDERR
###SBATCH -o /rhome/mdodson/testbed/slurm/out/slurm_grids_stats.out ## STDOUT
###SBATCH -e /rhome/mdodson/testbed/slurm/out/slurm_grids_stats.err ## STDERR
#SBATCH -o /rhome/mdodson/testbed/slurm/out/slurm_grids_eval-acclstm-4.out ## STDOUT
#SBATCH -e /rhome/mdodson/testbed/slurm/out/slurm_grids_eval-acclstm-4.err ## STDERR

module load cuda/11.4
set CUDA_PATH = /common/pkgs/cuda/cuda-11.4
${CUDA_PATH}/samples/bin/x86_64/linux/release/deviceQuery
### Set the mamba environment path to your environment
set mamba_env_path = /rhome/mdodson/.micromamba/envs/learn3
### Set dynamic link loader path variable to include CUDA and bins from mamba
setenv LD_LIBRARY_PATH ${LD_LIBRARY_PATH}:${mamba_env_path}/lib

nvidia-smi

cd /rhome/mdodson/testbed

set runcmd = /rhome/mdodson/.micromamba/envs/learn3/bin/python

${runcmd} -u testbed/eval_grids.py
