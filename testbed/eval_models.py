"""
:@module eval_model: This module contains methods and generators for evaluating
    models over both spatial (grid) and sequence domains, and storing the
    results as hdf5 files.

    --( gridded methods )--

    gen_gridded_predictions uses inputs and truth data returned by instances of
        generators.gen_timegrid_subgrid to run a model specified by a directory
        representing a tracktrain.ModelDir object.

    grid_preds_to_hdf5 takes the outputs of gen_gridded_predictions and stores
        them as a new HDF5 file ordered like (N, P, S, F) for N chronological
        runs of the model over each of P pixels in the grid. Each run has a
        sequence length S, and each array type has its own F features.

    gen_grid_prediction_combos chronologically generates timesteps from the
        prediction grid hdf5 files generated by grid_preds_to_hdf5.

    bulk_grid_error_stats_to_hdf5 utilizes the prediction combo generator to
        create grids of bulk statistics per each of the N model run time steps.

    gen_bulk_grid_stats yields error statistics by timestep from a bulk grid
    error stats file created by bulk_grid_error_stats_to_hdf5.

    --( sequence methods )--

    gen_sequence_predictions evaluates a model over the data yielded by a
        generators.sequence_dataset instance and yields the inputs, true
        values, and predictions together.

    sequence_preds_to_hdf5 stores the true values and predictions returned by
        gen_sequence_predictions in a hdf5 file ordered the same as the data
        yielded by the sequence_dataset generator.

    gen_sequence_prediction_combos takes a sequence hdf5 and a co-ordered
        sequence prediction hdf5 made by sequence_preds_to_hdf5 and yields
        the forcings and predictions together. This is the generator used by
        multiprocessed sequence evaluators

    --( multiprocessed sequence evaluators )--

    These evaluation methods are a separate pipeline than Evaluator objects
    used for sequence and gridded data in eval_sequences.py and eval_grids.py,
    although they are similar. They operate instead on sequence prediction
    files and their source sequence_dataset generators as yielded by
    gen_sequence_prediction_combos. These include the following methods and
    the single-threaded methods they wrap:

    mp_eval_error_horizons, mp_eval_joint_hists,
    mp_eval_temporal_error, mp_eval_static_error
"""
import copy
import gc
import json
import h5py
import numpy as np
import tensorflow as tf
from datetime import datetime
from multiprocessing import Pool
from pathlib import Path
from pprint import pprint
from time import perf_counter

import tracktrain as tt
from testbed import model_methods as mm
from testbed import generators
from testbed.list_feats import output_conversion_funcs

def gen_sequence_predictions(
        model_dir:tt.ModelDir, sequence_generator_args:dict,
        weights_file_name:str=None, gen_batch_size=256, max_batches=None,
        dynamic_norm_coeffs:dict={}, static_norm_coeffs:dict={},
        gen_numpy=False, output_conversion=None, reset_model_each_batch=False):
    """
    Evaluates the provided model on a series of sequence files, and generates
    the predictions alongside the inputs

    :@param model_dir: tracktrain.ModelDir object for the desired model.
    :@param sequence_generator_args: Dict of arguments sufficient to initialize
        a sequence dataset generator with generators.sequence_dataset().
        The full dict should be json-serializable (ex no Path objects) because
        it will be stored alongside the prediction data as an attribute in
        order to init similar generators in the future.
    :@param weights_file_name: Name of the model weights file from the ModelDir
        directory to be used for inference.
    :@param gen_batch_size: Number of samples to draw from the gen at once.
    :@param max_batches: Maximum number of gen batches to store in the file.
    :@param dyanmic_norm_coeffs: Dictionary mapping feature names to 2-tuples
        (mean,stdev) representing linear norm coefficients for dynamic feats.
    :@param static_norm_coeffs: Dictionary mapping feature names to 2-tuples
        (mean,stdev) representing linear norm coefficients for static feats.
    :@param gen_numpy: If True, generate numpy arrays instead of tensors
    :@param output_conversion: Tragically high-level option to convert beteween
        relative soil moisture (rsm) and soil moisture area density (soilm)
        in model outputs and true values. If not None, output_conversion must
        be either "rsm_to_soilm" or "soilm_to_rsm".
    :@param reset_model_each_batch: Some large custom models seem to overflow
        session memory for some reason when evaluated on many large batches.
        This option will reset the tensorflow session state and reload the
        model weights for each batch if set to True.
    """
    ## Generator loop expects times since they will be recorded in the file.
    assert sequence_generator_args.get("yield_times") == True
    ## Make sure the initial normalization is handled by the generator
    if "dynamic_norm_coeffs" not in sequence_generator_args.keys():
        print(f"WARNING: generator doesn't have dynamic norm coefficients" + \
                ", so those provided to gen_sequence_predictions are assumed")
        sequence_generator_args["dynamic_norm_coeffs"] = dynamic_norm_coeffs
    if "static_norm_coeffs" not in sequence_generator_args.keys():
        print(f"WARNING: generator doesn't have static norm coefficients" + \
                ", so those provided to gen_sequence_predictions are assumed")
        sequence_generator_args["static_norm_coeffs"] = static_norm_coeffs

    ## load the model weights
    if not weights_file_name is None:
        weights_file_name = Path(weights_file_name).name
    print(f"Loading weights")
    model = model_dir.load_weights(weights_path=weights_file_name)

    ## prepare to convert output units if requested
    target_outputs = None
    if output_conversion == "rsm_to_soilm":
        target_outputs = [
                f.replace("rsm","soilm")
                for f in sequence_generator_args["pred_feats"]
                ]
        do_conversion = target_outputs != sequence_generator_args["pred_feats"]
    elif output_conversion == "soilm_to_rsm":
        target_outputs = [
                f.replace("soilm", "rsm")
                for f in sequence_generator_args["pred_feats"]
                ]
        do_conversion = target_outputs != sequence_generator_args["pred_feats"]
    else:
        do_conversion = False

    if do_conversion:
        sequence_generator_args["static_feats"] += ["wiltingp", "porosity"]
        p_idxs,p_derived,_ = generators._parse_feat_idxs(
                out_feats=target_outputs,
                src_feats=sequence_generator_args["pred_feats"],
                static_feats=["wiltingp", "porosity"],
                derived_feats=output_conversion_funcs,
                )

    print("Declaring generator")
    ## ignore any conditions restricting training
    ## (now not ignoring since the sequence gen doesn't come from model config)
    #sequence_generator_args["static_conditions"] = []
    gen = generators.sequence_dataset(**sequence_generator_args)

    ## collect normalization coefficients
    w_norm = np.array([
        tuple(dynamic_norm_coeffs[k])
        if k in dynamic_norm_coeffs.keys() else (0,1)
        for k in model_dir.config["feats"]["window_feats"]
        ])[np.newaxis,:]
    h_norm = np.array([
        tuple(dynamic_norm_coeffs[k])
        if k in dynamic_norm_coeffs.keys() else (0,1)
        for k in model_dir.config["feats"]["horizon_feats"]
        ])[np.newaxis,:]
    s_norm = np.array([
        tuple(static_norm_coeffs[k])
        if k in static_norm_coeffs.keys() else (0,1)
        for k in model_dir.config["feats"]["static_feats"]
        ])[np.newaxis,:]
    p_norm = np.array([
        tuple(dynamic_norm_coeffs[k])
        if k in dynamic_norm_coeffs.keys() else (0,1)
        for k in model_dir.config["feats"]["pred_feats"]
        ])[np.newaxis,:]


    ## Separate out norm coeffs for output conversion
    if do_conversion:
        convert_norm = s_norm[:,-2:,:]
        s_norm = s_norm[:,:-2,:]
    batch_counter = 0
    max_batches = (max_batches, -1)[max_batches is None]
    for (w,h,s,si,t),ys in gen.batch(gen_batch_size):
        print(f"Recieved new batch")
        if do_conversion:
            sparams = s[...,-2:]
            s = s[...,:-2]
            sparams = sparams * convert_norm[...,1] + convert_norm[...,0]

        if reset_model_each_batch:
            tf.keras.backend.clear_session()
            model = model_dir.load_weights(weights_path=weights_file_name)

        ## Normalize the predictions (assumes add_norm_layers not used!!!)
        print(f"Executing model")
        pr = model((w,h,s,si)) * p_norm[...,1]

        print(f"Calculating feature arrays")
        ## retain the initial observed state so the residual can be accumulated
        w = w * w_norm[...,1] + w_norm[...,0]
        h = h * h_norm[...,1] + h_norm[...,0]
        s = s * s_norm[...,1] + s_norm[...,0]
        ys = ys * p_norm[...,1] + p_norm[...,0]

        ## use the calculated functional parameters to convert units if needed
        if do_conversion:
            ps = tf.concat(
                    (ys[:,0,:][:,tf.newaxis,:],
                        (ys[:,0,:][:,tf.newaxis,:] + tf.cumsum(pr, axis=1))),
                    axis=1
                    )
            ps = generators._calc_feat_array(
                    src_array=ps,
                    static_array=sparams[:,np.newaxis],
                    stored_feat_idxs=tf.convert_to_tensor(p_idxs),
                    derived_data=p_derived,
                    )
            pr = ps[:,1:] - ps[:,:-1]
            ys = generators._calc_feat_array(
                    src_array=ys,
                    static_array=sparams[:,np.newaxis],
                    stored_feat_idxs=tf.convert_to_tensor(p_idxs),
                    derived_data=p_derived,
                    )

        th = t[:,-pr.shape[1]:]

        print(f"Generating result")
        if gen_numpy:
            w = w.numpy()
            h = h.numpy()
            s = s.numpy()
            si = si.numpy()
            th = th.numpy()
            ys = ys.numpy()
            pr = pr.numpy()
        batch_counter += 1
        if  batch_counter == max_batches:
            break
        yield (w,h,s,si,th),ys,pr

def sequence_preds_to_hdf5(model_dir:tt.ModelDir, sequence_generator_args:dict,
        pred_h5_path:Path, weights_file_name:str=None, chunk_size=256,
        gen_batch_size=256, max_batches=None,
        dynamic_norm_coeffs:dict={}, static_norm_coeffs:dict={}):
    """
    Evaluates the provided model on a series of sequence files, and stores the
    predictions in a new hdf5 file with a float32 dataset shaped (N,S,F_p) for
    N samples of length S sequences having F_p predicted features, and a uint
    dataset of epoch times shaped (N,S).

    :@param model_dir: tracktrain.ModelDir object for the desired model.
    :@param sequence_generator_args: Dict of arguments sufficient to initialize
        a sequence dataset generator with generators.sequence_dataset().
        The full dict should be json-serializable (ex no Path objects) because
        it will be stored alongside the prediction data as an attribute in
        order to init similar generators in the future.
    :@param pred_h5_path: path to a new hdf5 file storing predictions. Should
        conform to format "pred_{region}_{season}_{timerange}_{model}.h5"
    :@param weights_file_name: Name of the model weights file from the ModelDir
        directory to be used for inference.
    :@param chunk_size: Number of samples per chunk in the new hdf5
    :@param gen_batch_size: Number of samples to draw from the gen at once.
    :@param max_batches: Maximum number of gen batches to store in the file.
    :@param dynamic_norm_coeffs: Dictionary mapping feature names to 2-tuples
        (mean,stdev) representing linear norm coefficients for dynamic feats.
    :@param static_norm_coeffs: Dictionary mapping feature names to 2-tuples
        (mean,stdev) representing linear norm coefficients for static feats.
    """
    ## Initialize a prediction generator with the provided parameters
    gen = gen_sequence_predictions(
            model_dir=model_dir,
            sequence_generator_args=sequence_generator_args,
            weights_file_name=weights_file_name,
            gen_batch_size=gen_batch_size,
            max_batches=max_batches,
            dynamic_norm_coeffs=dynamic_norm_coeffs,
            static_norm_coeffs=static_norm_coeffs,
            )
    h5idx = 0
    batch_counter = 0
    max_batches = (max_batches, -1)[max_batches is None]
    F = None
    for i,((w,h,s,si,th),ys,pr) in enumerate(gen):
        if F is None:
            ## Create a new h5 file with datasets for the model (residual)
            ## predictions, timesteps, and initial states (last pred feats).
            F = h5py.File(
                    name=pred_h5_path,
                    mode="w-",
                    rdcc_nbytes=128*1024**2, ## use a 128MB cache
                    )
            output_shape = pr.shape[1:]
            P = F.create_dataset(
                    name="/data/preds",
                    shape=(0, *output_shape),
                    maxshape=(None, *output_shape),
                    chunks=(chunk_size, *output_shape),
                    compression="gzip",
                    )
            ## Times only include prediction horizon timestamps
            T = F.create_dataset(
                    name="/data/time",
                    shape=(0, output_shape[0]),
                    maxshape=(None, output_shape[0]),
                    chunks=(chunk_size, output_shape[0]),
                    compression="gzip"
                    )
            Y0 = F.create_dataset(
                    name="/data/init_states",
                    shape=(0, output_shape[-1]),
                    maxshape=(None, output_shape[0]),
                    chunks=(chunk_size, output_shape[0]),
                    compression="gzip",
                    )
            ## Store the generator arguments so the same kind can be re-init'd
            F["data"].attrs["gen_args"] = json.dumps(sequence_generator_args)
            F["data"].attrs["weights_file"] = Path(weights_file_name).name

        sample_slice = slice(h5idx, h5idx+ys.shape[0])
        h5idx += ys.shape[0]
        ## Only store initial state in the hdf5
        y0 = ys[:,0,:]
        ## Make room and dump the data to the file
        P.resize((h5idx, *pr.shape[1:]))
        T.resize((h5idx, th.shape[-1]))
        Y0.resize((h5idx, y0.shape[-1]))
        P[sample_slice,...] = pr.numpy()
        T[sample_slice,...] = th.numpy()
        Y0[sample_slice,...] = np.reshape(
                y0.numpy(),(y0.shape[0],y0.shape[-1]))
        F.flush()
        gc.collect()
        print(f"Loaded batch {i}; ({sample_slice.start}-{sample_slice.stop})")
    F.close()
    return pred_h5_path

def eval_error_horizons(sequence_h5, prediction_h5,
        batch_size=1024, buf_size_mb=128):
    """
    Calculate the state and residual error and approximate variance with
    respect to each forecast horizon in a sequence/prediction hdf5 pair
    """
    param_dict = generators.parse_sequence_params(sequence_h5)
    pred_dict = generators.parse_prediction_params(prediction_h5)
    print(f"horizons {prediction_h5.name}")
    coarseness = pred_dict.get("pred_coarseness", 1)
    gen = generators.gen_sequence_prediction_combos(
            seq_h5=sequence_h5,
            pred_h5=prediction_h5,
            batch_size=batch_size,
            buf_size_mb=buf_size_mb,
            pred_coarseness=coarseness
            )
    counts = None
    es_sum = None
    er_sum = None
    es_var_sum = None
    er_var_sum = None
    for (_, (ys, pr)) in gen:
        ## the predicted state time series
        ps = ys[:,0,:][:,np.newaxis,:] + np.cumsum(pr, axis=1)
        ## Calculate the label residual from labels
        yr = ys[:,1:]-ys[:,:-1]
        ## Calculate the error in the residual and state predictions
        es = ps - ys[:,1:,:]
        er = pr - yr

        es_abs = np.abs(es)
        er_abs = np.abs(er)

        ## Calculate the average absolute error and approximate variance
        ## (approximate since early samples are computed w/ partial avg)
        if counts is None:
            counts = es_abs.shape[0]
            es_sum = np.sum(es_abs, axis=0)
            er_sum = np.sum(er_abs, axis=0)
            es_var_sum = np.sum((es_abs - es_sum/counts)**2, axis=0)
            er_var_sum = np.sum((er_abs - er_sum/counts)**2, axis=0)
        else:
            counts += es_abs.shape[0]
            es_sum += np.sum(es_abs, axis=0)
            er_sum += np.sum(er_abs, axis=0)
            es_var_sum += np.sum((es_abs - es_sum/counts)**2, axis=0)
            er_var_sum += np.sum((er_abs - er_sum/counts)**2, axis=0)
    return {
            "state_avg":es_sum/counts,
            "state_var":es_var_sum/counts,
            "residual_avg":er_sum/counts,
            "residual_var":er_var_sum/counts,
            "counts":counts,
            "feats":pred_dict["pred_feats"],
            "pred_coarseness":coarseness,
            }

def mp_eval_error_horizons(args:tuple):
    return eval_error_horizons(*args)

def eval_joint_hists(
        sequence_h5:Path, prediction_h5:Path,
        pred_state_bounds:dict, pred_residual_bounds:dict, num_bins:int,
        batch_size=1024, buf_size_mb=128, horizon_limit=None,
        get_state_hist=True, get_residual_hist=True):
    """
    Returns (NY,NP,F_p) shaped integer grids of sample magnitude counts binned
    according to each feature's bounds, constructed like a validation grid or a
    joint histogram of label and predicted values such that NY == NP.

    returns a dict formatted like:
    {
        "state_hist":3D array of state counts with label values on the 1st axis
        "state_bounds"2-tuple (mins,maxs) of 1d arrays for state feats
        "residual_hist":3D array of residual counts with labels on the 1st axis
        "residual_bounds":2-tuple (mins,maxs) of 1d arrays for residual feats
    }

    :@param sequence_h5: Path to a sequence hdf5 generated by
        generators.sequence_dataset, having an order that matches the
        provided prediction hdf5
    :@param prediction_h5: Path to a prediction hdf5 from
        sequence_preds_to_hdf5 ordered identically to the provided seq hdf5
    :@param pred_state_bounds: Dictionary mapping prediction feature names to
        inclusive 2-tuple [min,max] bounds for the histogram magnitude extent.
    :@param pred_residual_bounds: Dictionary mapping prediction feature names
        to 2-tuple [min,max] bounds for the histogram magnitude extent.
    :@param num_bins: Number of bins in the histogram between min and max
    :@param batch_size: Number of samples to simultaneously evaluate
    :@param buf_size_mb: hdf5 chunk buffers size. Probably not important here.
    :@param horizon_limit: Maximum number of horizon steps to include in the
        histogram analysis; useful for excluding diverging state values
    :@param get_state_hist: When True, value counts are collected for state
        magnitudes, and included in the returned dictionary.
    :@param get_residual_hist: When True, value counts are collected for
        residual magnitudes, and included in the returned dictionary.
    """
    param_dict = generators.parse_sequence_params(sequence_h5)
    pred_dict = generators.parse_prediction_params(prediction_h5)
    coarseness = pred_dict.get("pred_coarseness", 1)
    print(f"hists {prediction_h5.name}")
    smins,smaxs = zip(*[
        pred_state_bounds[k]
        for k in pred_dict["pred_feats"]
        ])
    rmins,rmaxs = zip(*[
        pred_residual_bounds[k]
        for k in pred_dict["pred_feats"]
        ])
    rmins,rmaxs,smins,smaxs = map(np.array, (rmins, rmaxs, smins, smaxs))
    if horizon_limit is None:
        horizon_limit = param_dict["horizon_size"]

    def _norm_to_idxs(A:np.array, mins, maxs):
        A = (np.clip(A, mins, maxs) - mins) / (maxs - mins)
        A = np.clip(np.floor(A * num_bins), 0, num_bins-1).astype(np.uint32)
        return A

    gen = generators.gen_sequence_prediction_combos(
            seq_h5=sequence_h5,
            pred_h5=prediction_h5,
            batch_size=batch_size,
            buf_size_mb=buf_size_mb,
            pred_coarseness=coarseness,
            )
    r_counts = np.zeros((num_bins,num_bins,rmins.size), dtype=np.uint32)
    s_counts = np.zeros((num_bins,num_bins,smins.size), dtype=np.uint32)
    for (_, (ys, pr)) in gen:
        if get_state_hist:
            ## accumulate the predicted state time series
            ps = ys[:,0,:][:,np.newaxis,:] + np.cumsum(pr, axis=1)

            ys_idxs = np.reshape(
                    _norm_to_idxs(ys[:,1:1+horizon_limit], smins, smaxs),
                    (-1, smins.size))
            ps_idxs = np.reshape(
                    _norm_to_idxs(ps[:,:horizon_limit], smins, smaxs),
                    (-1, smins.size))
            ## Loop since fancy indexing doesn't accumulate repetitions
            for i in range(ys_idxs.shape[0]):
                for j in range(ys_idxs.shape[-1]):
                    s_counts[ys_idxs[i,j],ps_idxs[i,j],j] += 1

        if get_residual_hist:
            ## Calculate the label residual from labels
            yr = ys[:,1:]-ys[:,:-1]
            yr_idxs = np.reshape(
                    _norm_to_idxs(yr[:,1:1+horizon_limit], rmins, rmaxs),
                    (-1, rmins.size))
            pr_idxs = np.reshape(
                    _norm_to_idxs(pr[:,:horizon_limit], rmins, rmaxs),
                    (-1, rmins.size))
            ## Loop since fancy indexing doesn't accumulate repetitions
            for i in range(yr_idxs.shape[0]):
                for j in range(yr_idxs.shape[-1]):
                    r_counts[yr_idxs[i,j],pr_idxs[i,j],j] += 1
    result = {}
    if get_state_hist:
        result["state_hist"] = s_counts
        result["state_bounds"] = (smins, smaxs)
    if get_residual_hist:
        result["residual_hist"] = r_counts
        result["residual_bounds"] = (rmins, rmaxs)
    result["feats"] = pred_dict["pred_feats"]
    return result

def mp_eval_joint_hists(kwargs:dict):
    return eval_joint_hists(**kwargs)

def eval_temporal_error(sequence_h5, prediction_h5,
        batch_size=1024, buf_size_mb=128, horizon_limit=None,
        absolute_error=True):
    """
    Calculate the state and residual error rates of each predicted feature
    with respect to the hourly time of day and the day of year given matching
    sequence input and prediction output hdf5s.

    :@param sequence_h5: Path to a sequence hdf5 generated by
        generators.sequence_dataset, having an order that matches the
        provided prediction hdf5
    :@param prediction_h5: Path to a prediction hdf5 from
        sequence_preds_to_hdf5 ordered identically to the provided seq hdf5
    :@param batch_size: Number of samples to simultaneously evaluate
    :@param buf_size_mb: hdf5 chunk buffers size. Probably not important here.
    :@param horizon_limit: Maximum number of horizon steps to include in the
        histogram analysis; useful for excluding diverging state values
    :@param absolute_error: When True, error magnitudes are absolute, so they
        don't cancel each other out when over and underestimated. When False,
        the errors' actual magnitude (and thus the bias) are returned.
    """
    pred_dict = generators.parse_prediction_params(prediction_h5)
    coarseness = pred_dict.get("pred_coarseness", 1)
    print(f"temporal {prediction_h5.name}")
    gen = generators.gen_sequence_prediction_combos(
            seq_h5=sequence_h5,
            pred_h5=prediction_h5,
            batch_size=batch_size,
            buf_size_mb=buf_size_mb,
            gen_times=True,
            pred_coarseness=coarseness,
            )
    param_dict = generators.parse_sequence_params(sequence_h5)
    if horizon_limit is None:
        horizon_limit = param_dict["horizon_size"]

    doy_r = np.zeros((366, len(pred_dict["pred_feats"])))
    doy_s = np.zeros((366, len(pred_dict["pred_feats"])))
    doy_counts = np.zeros((366, len(pred_dict["pred_feats"])), dtype=np.uint)
    tod_r = np.zeros((24, len(pred_dict["pred_feats"])))
    tod_s = np.zeros((24, len(pred_dict["pred_feats"])))
    tod_counts = np.zeros((24, len(pred_dict["pred_feats"])), dtype=np.uint)
    for ((_,_,_,_,(yt,pt)), (ys, pr)) in gen:
        ps = ys[:,0,:][:,np.newaxis,:] + np.cumsum(pr, axis=1)
        yr = ys[:,1:]-ys[:,:-1]

        times = list(map(
            datetime.fromtimestamp,
            pt.astype(np.uint)[:,:horizon_limit].reshape((-1,))
            ))
        ## Times are reported exactly on the hour, but float rounding can cause
        ## some to be above or below. Add a conditional to account for this.
        tmp_tods = np.array([
            (t.hour+1 if t.minute >= 30 else t.hour)%24 for t in times
            ])
        tmp_doys = np.array([t.timetuple().tm_yday-1 for t in times])

        es = ps - ys[:,1:]
        er = pr - yr
        if absolute_error:
            es,er = map(np.abs,(es,er))
        es = es[:,:horizon_limit].reshape((-1, es.shape[-1]))
        er = er[:,:horizon_limit].reshape((-1, er.shape[-1]))

        for i in range(len(times)):
            doy_s[tmp_doys[i]] += es[i]
            doy_r[tmp_doys[i]] += er[i]
            doy_counts[tmp_doys[i]] += 1
            tod_s[tmp_tods[i]] += es[i]
            tod_r[tmp_tods[i]] += er[i]
            tod_counts[tmp_tods[i]] += 1
    return {
            "doy_state":doy_s,
            "doy_residual":doy_r,
            "doy_counts":doy_counts,
            "tod_state":tod_s,
            "tod_residual":tod_r,
            "tod_counts":tod_counts,
            "feats":pred_dict["pred_feats"],
            }

def mp_eval_temporal_error(kwargs:dict):
    return eval_temporal_error(**kwargs)

def eval_static_error(sequence_h5, prediction_h5,
        batch_size=1024, buf_size_mb=128):
    """
    Calculate the state and residual error rates of each predicted feature
    with respect to the hourly time of day and the day of year given matching
    sequence input and prediction output hdf5s.

    :@param sequence_h5: Path to a sequence hdf5 generated by
        generators.sequence_dataset, having an order that matches the
        provided prediction hdf5
    :@param prediction_h5: Path to a prediction hdf5 from
        sequence_preds_to_hdf5 ordered identically to the provided seq hdf5
    :@param batch_size: Number of samples to simultaneously evaluate
    :@param buf_size_mb: hdf5 chunk buffers size. Probably not important here.
    """
    print(f"static {prediction_h5.name}")
    pred_dict = generators.parse_prediction_params(prediction_h5)
    param_dict = generators.parse_sequence_params(sequence_h5)
    coarseness = pred_dict.get("pred_coarseness", 1)
    gen = generators.gen_sequence_prediction_combos(
            seq_h5=sequence_h5,
            pred_h5=prediction_h5,
            batch_size=batch_size,
            buf_size_mb=buf_size_mb,
            gen_static=True,
            gen_static_int=True,
            pred_coarseness=coarseness,
            )
    ## Gather the indeces of generated soil texture static features
    soil_idxs = tuple(
            param_dict["static_feats"].index(l)
            for l in ("pct_sand", "pct_silt", "pct_clay"))

    ## Soil components to index mapping. Scuffed and slow, I know, but
    ## unfortunately I didn't store integer types alongside sequence samples,
    ## and it's too late to turn back now :(
    soil_mapping = list(map(
        lambda a:np.array(a, dtype=np.float32),
        [
            [0.,   0.,   0.  ],
            [0.92, 0.05, 0.03],
            [0.82, 0.12, 0.06],
            [0.58, 0.32, 0.1 ],
            [0.17, 0.7 , 0.13],
            [0.1 , 0.85, 0.05],
            [0.43, 0.39, 0.18],
            [0.58, 0.15, 0.27],
            [0.1 , 0.56, 0.34],
            [0.32, 0.34, 0.34],
            [0.52, 0.06, 0.42],
            [0.06, 0.47, 0.47],
            [0.22, 0.2 , 0.58],
            ]
        ))

    ## count and error sum matrices shaped for (vegetation, soil)
    counts = np.zeros((14,13))
    err_res = np.zeros((14,13,len(pred_dict["pred_feats"])))
    err_state = np.zeros((14,13,len(pred_dict["pred_feats"])))
    for ((_,_,s,si,_), (ys, pr)) in gen:
        ## the predicted state time series
        ps = ys[:,0,:][:,np.newaxis,:] + np.cumsum(pr, axis=1)
        ## Calculate the label residual from labels
        yr = ys[:,1:]-ys[:,:-1]
        ## Calculate the error in the residual and state predictions
        es = ps - ys[:,1:,:]
        er = pr - yr

        ## Average the error over the full horizon
        es_abs = np.average(np.abs(es), axis=1)
        er_abs = np.average(np.abs(er), axis=1)

        soil_texture = s[...,soil_idxs]
        for i,soil_array in enumerate(soil_mapping):
            ## Get a boolean mask
            m_this_soil = (soil_texture == soil_array).all(axis=1)
            if not np.any(m_this_soil):
                continue
            es_abs_subset = es_abs[m_this_soil]
            er_abs_subset = er_abs[m_this_soil]
            si_subset = si[m_this_soil]
            ## Convert the one-hot encoded vegetation vectors to indeces
            si_idxs = np.argwhere(si_subset)[:,1]
            for j in range(si_idxs.shape[0]):
                err_res[si_idxs[j],i] += er_abs_subset[j]
                err_state[si_idxs[j],i] += es_abs_subset[j]
                counts[si_idxs[j],i] += 1
    return {
            "err_state":err_state,
            "err_residual":err_res,
            "counts":counts,
            "feats":pred_dict["pred_feats"],
            }
def mp_eval_static_error(args:tuple):
    return eval_static_error(*args)

def pearson_coeff(y, p, axis=1, keepdims=True):
    """ Calculate the pearson coefficient of sequences along an axis """
    y_mdiff = y - np.average(y, axis=axis, keepdims=keepdims)
    p_mdiff = p - np.average(p, axis=axis, keepdims=keepdims)
    num = np.sum( (y-y_mdiff)*(p-p_mdiff), axis=axis, keepdims=keepdims)
    y_denom = np.sum(y_mdiff**2, axis=axis, keepdims=keepdims)**(1/2)
    p_denom = np.sum(p_mdiff**2, axis=axis, keepdims=keepdims)**(1/2)
    return num / (y_denom * p_denom)

""" ------------( Start gridded data methods )------------ """

def gen_gridded_predictions(model_dir:tt.ModelDir, grid_generator_args:dict,
        weights_file_name=None, m_valid=None, dynamic_norm_coeffs:dict={},
        static_norm_coeffs:dict={}, yield_normed_inputs:bool=False,
        yield_normed_outputs:bool=False, output_conversion=None,
        debug=False):
    """
    Generates model inputs, true outputs, and model predictions as a 3-tuple.

    Executes a trained model (given its ModelDir object) over the data
    returned by a generators.gen_timegrid_subgrids initialized with
    grid_generator_args.

    :@param model_dir: ModelDir object associated with the model to run
    :@param grid_generator_args: JSON-serializable dict of arguments sufficient
        to initialize a generators.gen_timegrid_subgrids as a dataset.
    :@param weights_file_name: String name of the weights file within the
        provided ModelDir to use for inference. Defualts to _final.weights.h5
    :@param dynamic_norm_coeffs: Dict containing feature names mapped to
        (mean,stdev) dynamic normalization coefficients.
    :@param static_norm_coeffs: Dict containing feature names mapped to
        (mean,stdev) static normalization coefficients.
    """
    model = model_dir.load_weights(weights_path=weights_file_name)
    ## extract the coarseness in order to sub-sample the times
    coarseness = model_dir.config.get("feats").get("pred_coarseness", 1)

    ## things can get weird when modifying the dict if ya don't copy
    gga = copy.deepcopy(grid_generator_args)
    ## prepare to convert output units if requested
    target_outputs = None
    if output_conversion == "rsm_to_soilm":
        target_outputs = [
                f.replace("rsm","soilm")
                for f in gga["pred_feats"]
                ]
        do_conversion = target_outputs != gga["pred_feats"]
    elif output_conversion == "soilm_to_rsm":
        target_outputs = [
                f.replace("soilm", "rsm")
                for f in gga["pred_feats"]
                ]
        do_conversion = target_outputs != gga["pred_feats"]
    else:
        do_conversion = False
    if do_conversion:
        gga["static_feats"] += ["wiltingp", "porosity"]
        p_idxs,p_derived,_ = generators._parse_feat_idxs(
                out_feats=target_outputs,
                src_feats=gga["pred_feats"],
                static_feats=["wiltingp", "porosity"],
                derived_feats=output_conversion_funcs,
                )

    ## declare a grid generator for this region/model combination
    gen_tg = generators.gen_timegrid_subgrids(**gga)

    ## collect normalization coefficients
    w_norm = np.array([
        tuple(dynamic_norm_coeffs[k])
        if k in dynamic_norm_coeffs.keys() else (0,1)
        for k in model_dir.config["feats"]["window_feats"]
        ])[np.newaxis,:]
    h_norm = np.array([
        tuple(dynamic_norm_coeffs[k])
        if k in dynamic_norm_coeffs.keys() else (0,1)
        for k in model_dir.config["feats"]["horizon_feats"]
        ])[np.newaxis,:]
    ## use static feats listing here since conversion parameters may be added.
    s_norm = np.array([
        tuple(static_norm_coeffs[k])
        if k in static_norm_coeffs.keys() else (0,1)
        for k in gga["static_feats"]
        ])[np.newaxis,:]
    p_norm = np.array([
        tuple(dynamic_norm_coeffs[k])
        if k in dynamic_norm_coeffs.keys() else (0,1)
        for k in model_dir.config["feats"]["pred_feats"]
        ])[np.newaxis,:]

    F = None
    ix = None
    h5_idx = 0
    ## Separate out norm coeffs for output conversion
    if do_conversion:
        convert_norm = s_norm[:,-2:,:]
        s_norm = s_norm[:,:-2,:]
    for (w,h,s,si,t),ys in gen_tg:
        ## Extract the boolean mask for valid pixels
        if m_valid is None:
            m_valid = np.full(s.shape[:-1], True)
        if do_conversion:
            sparams = s[...,-2:]
            s = s[...,:-2]
            sparams = sparams * convert_norm[...,1] + convert_norm[...,0]
        ## Get a spatial subset of the valid mask according to the generator
        ## argument dict's spatial bounds and a provided full-size valid mask.
        if ix is None:
            hslice = slice(
                    grid_generator_args.get("hidx_min"),
                    grid_generator_args.get("hidx_max"))
            vslice = slice(
                    grid_generator_args.get("vidx_min"),
                    grid_generator_args.get("vidx_max"))
            m_valid = m_valid[vslice,hslice]
            ix = np.stack(np.where(m_valid), axis=-1)

        ## Apply the mask and organize the batch axis across pixels
        ys = ys[:,m_valid].transpose((1,0,2))
        w = (w[:,m_valid].transpose((1,0,2))-w_norm[...,0])/w_norm[...,1]
        h = (h[:,m_valid].transpose((1,0,2))-h_norm[...,0])/h_norm[...,1]
        s = (s[m_valid]-s_norm[...,0])/s_norm[...,1]
        si = si[m_valid]

        ## evaluate the model on the inputs and rescale the results
        if debug:
            clock_0 = perf_counter()
            pr = model((w,h,s,si))
            clock_f = perf_counter()
            tmp_time = datetime.fromtimestamp(int(t[w.shape[1]]))
            tmp_time = tmp_time.strftime("%Y%m%d %H%M")
            tmp_dt = f"{clock_f-clock_0:.3f}"
            print(f"{tmp_time} evaluated {pr.shape[0]} px in {tmp_dt} sec")
        else:
            pr = model((w,h,s,si))

        ## invert the residual's normalization scale if requested
        if yield_normed_outputs:
            ys = (ys-p_norm[...,0])/p_norm[...,1]
        else:
            pr = pr * p_norm[...,1]
        if not yield_normed_inputs:
            w = w * w_norm[...,1] + w_norm[...,0]
            h = h * h_norm[...,1] + h_norm[...,0]
            s = s * s_norm[...,1] + s_norm[...,0]

        ## use the calculated functional params to convert units if needed
        if do_conversion:
            sparams = sparams[m_valid]
            ps = np.concatenate(
                    (ys[:,0,:][:,np.newaxis,:],
                        (ys[:,0,:][:,np.newaxis,:] + np.cumsum(pr, axis=1))),
                    axis=1
                    )
            ps = generators._calc_feat_array(
                    src_array=ps,
                    static_array=sparams[:,np.newaxis],
                    stored_feat_idxs=p_idxs,
                    derived_data=p_derived,
                    )
            pr = ps[:,1:] - ps[:,:-1]
            ys = generators._calc_feat_array(
                    src_array=ys,
                    static_array=sparams[:,np.newaxis],
                    stored_feat_idxs=p_idxs,
                    derived_data=p_derived,
                    )

        ## subsample times to the output coarseness of this model,
        ## and restrict t to only span the output sequence range
        #ys = ys[:,::coarseness,:]
        t = t[-pr.shape[1]:][::coarseness]

        ## yield 3-tuple of  inputs, true values, model predictions, indeces
        yield (w,h,s,si,t),ys,pr,ix


def grid_preds_to_hdf5(model_dir:tt.ModelDir, grid_generator_args:dict,
        pred_h5_path:Path, weights_file_name:str=None, pixel_chunk_size=64,
        sample_chunk_size=32, dynamic_norm_coeffs={}, static_norm_coeffs={},
        yield_normed_inputs=False, yield_normed_outputs=False,
        save_window=False, save_horizon=False, save_static=False,
        save_static_int=False, extract_valid_mask:bool=False, debug=False):
    """
    Evaluate a trained model on spatial grids on data from a
    generators.gen_timegrid_subgrid, and save the predictions, true values,
    timestamps, and spatial grid indeces separately over multiple init times.

    Predictions and true values are stored as (T,P,S,F) shaped arrays such that
    T : Timestep , P : Pixel (valid only) , S : Sequence step , F : pred feat

    /data/preds     : (N, P, S_p, F_p)  Model residual outputs
    /data/truth     : (N, P, S_y, F_p)  True state outputs
    /data/time      : (N, S_p)          Integer epoch times
    /data/idxs      : (P, 2)            Integer indeces
    /data/window    : (N, P, S_w, F_w)  Window inputs
    /data/horizon   : (N, P, S_p, F_h)  Horizon inputs
    /data/static    : (P, F_s)          Static inputs
    /data/static_int: (P, F_si)         Static integer inputs

    :@param model_dir: ModelDir object associated with the model to run
    :@param grid_generator_args: JSON-serializable dict of arguments sufficient
        to initialize a generators.gen_timegrid_subgrids as a dataset.
    :@param pred_h5_path: Path to a non-existent hdf5 file that will be written
        with the results from each sample timestep of this model.
    :@param weights_file_name: String name of the weights file within the
        provided ModelDir to use for inference. Defualts to _final.weights.h5
    :@param pixel_chunk_size: Number of elements per chunk along the pixel axis
    :@param sample_chunk_size: Number of samples per chunk along the first axis
    :@param dynamic_norm_coeffs: Dict containing prediction coefficient names
        mapped to (mean,stdev) normalization coefficients
    :@param save_*: If True, save the corresponding dataset in the hdf5
    :@param extract_valid_mask: If True, establishes the valid mask for the
        whole timegrid series by extracting the static parameter labeled
        "m_valid" from the first provided file.
    """
    ## apply valid mask labeled m_valid if key found in timegrid attributes
    tmpf = h5py.File(name=grid_generator_args["timegrid_paths"][0], mode="r")
    if extract_valid_mask:
        _,_,tg_static_args = generators.parse_timegrid_attrs(
                grid_generator_args["timegrid_paths"][0])
        mask_idx = tg_static_args["flabels"].index("m_valid")
        m_valid = tmpf["/data/static"][...,mask_idx].astype(bool)
    else:
        m_valid = np.full(tmpf["/data/static"][...,0].shape, True)


    gen = gen_gridded_predictions(
            model_dir=model_dir,
            grid_generator_args=grid_generator_args,
            weights_file_name=weights_file_name,
            dynamic_norm_coeffs=dynamic_norm_coeffs,
            static_norm_coeffs=static_norm_coeffs,
            yield_normed_inputs=yield_normed_inputs,
            yield_normed_outputs=yield_normed_outputs,
            m_valid=m_valid,
            debug=debug,
            )

    F = None
    h5_idx = 0
    for (w,h,s,si,t),y,p,ix in gen:
        ## initialize the file if it hasn't already been created
        if F is None:
            ## Create a new h5 file with datasets for the model (residual)
            ## predictions, true (state) values, and timesteps
            #valid_idxs = np.stack(np.where(m_valid), axis=-1)
            F = h5py.File(
                    name=pred_h5_path,
                    mode="w-",
                    ## use a 256MB cache (shouldn't matter)
                    rdcc_nbytes=256*1024**2,
                    )
            ## (N, P, S_p, F) Predicted values
            chunks = (sample_chunk_size,pixel_chunk_size)
            P = F.create_dataset(
                    name="/data/preds",
                    shape=(0, *p.shape),
                    maxshape=(None, *p.shape),
                    chunks=(*chunks,*p.shape[1:]),
                    compression="gzip",
                    )
            ## (N, P, S_y, F) True values
            Y = F.create_dataset(
                    name="/data/truth",
                    shape=(0, *y.shape),
                    maxshape=(None, *y.shape),
                    chunks=(*chunks, *y.shape[1:]),
                    compression="gzip",
                    )
            ## (N, S_p) Epoch times
            T = F.create_dataset(
                    name="/data/time",
                    shape=(0, y.shape[1]),
                    maxshape=(None, y.shape[1]),
                    compression="gzip",
                    )
            if save_window:
                W = F.create_dataset(
                        name="/data/window",
                        shape=(0, *w.shape),
                        maxshape=(None, *w.shape),
                        chunks=(*chunks, *w.shape[1:]),
                        compression="gzip",
                        )
            if save_horizon:
                H = F.create_dataset(
                        name="/data/horizon",
                        shape=(0, *h.shape),
                        maxshape=(None, *h.shape),
                        chunks=(*chunks, *h.shape[1:]),
                        compression="gzip",
                        )
            if save_static:
                S = F.create_dataset(
                        name="/data/static",
                        shape=s.shape,
                        maxshape=s.shape,
                        )
                S[...] = s
            if save_static_int:
                SI = F.create_dataset(
                        name="/data/static_int",
                        shape=si.shape,
                        maxshape=si.shape,
                        )
                SI[...] = si

            ## (P, 2) Valid pixel indeces
            IDX = F.create_dataset(
                    name="/data/idxs",
                    shape=ix.shape,
                    maxshape=ix.shape,
                    compression="gzip",
                    )
            ## Go ahead and load the indeces
            IDX[...] = ix
            ## Store the generator arguments so the same can be re-initialized
            grid_generator_args["timegrid_paths"] = [
                    Path(p).as_posix()
                    for p in grid_generator_args["timegrid_paths"]
                    ]
            F["data"].attrs["gen_args"] = json.dumps(grid_generator_args)
            F["data"].attrs["grid_shape"] = np.array(m_valid.shape)
            F["data"].attrs["model_config"] = json.dumps(model_dir.config)

        ## Incrementally expand the file and load data per timestep sample
        P.resize((h5_idx+1, *p.shape))
        Y.resize((h5_idx+1, *y.shape))
        T.resize((h5_idx+1, *t.shape))
        P[h5_idx,...] = p
        Y[h5_idx,...] = y
        T[h5_idx,...] = t

        ## Rescale and save additional datasets that were requested for the h5
        if save_window:
            W.resize((h5_idx+1, *w.shape))
            W[h5_idx,...] = w
        if save_horizon:
            H.resize((h5_idx+1, *h.shape))
            H[h5_idx,...] = h
        h5_idx += 1
    F.close()
    return pred_h5_path

def mp_grid_preds_to_hdf5(kwargs):
    """
    Helper method for multiprocessing over grid_preds_to_hdf5.

    ModelDir objects are initialized here because the custom_model_builders
    methods cannot be serialized for multiprocessing. Thus, kwargs['model_dir']
    is expected to be the string path of the model directory rather than the
    corresponding ModelDir object, unlike grid_preds_to_hdf5.

    custom_model_builders here must be kept up to date.

    :@param kwargs: dictionary of keyword arguments to
        eval_grids.grid_preds_to_hdf5. These are identical to the arguments
        passed to the method, EXCEPT model_dir must be the Path representing
        the ModelDir object.
    """
    kwargs["model_dir"] = tt.ModelDir(
            kwargs["model_dir"],
            custom_model_builders={
                "lstm-s2s":lambda args:mm.get_lstm_s2s(**args),
                "acclstm":lambda args:mm.get_acclstm(**args),
                },
            )
    return grid_preds_to_hdf5(**kwargs)


def gen_grid_prediction_combos(grid_h5:Path):
    """
    Simple generator returning gridded sequence predictions one-by-one from a
    hdf5 file populated by grid_preds_to_hdf5.

    Sequences are yielded per timestep as 4-tuples (true, pred, idxs, time) st:
    true: (P, S, F_p) P valid pixels having S sequence members and F_p feats
    pred: (P, S, F_p) Predictions associted directly with the true sequences
    idxs: (P, 2)      Indeces of each of the P valid pixels on a larger grid
    time: float       Epoch time of the currently yielded timestep's pivot time
                      (the first predicted timestep)
    """
    with h5py.File(grid_h5, mode="r") as grid_file:
        P = grid_file["/data/preds"]
        Y = grid_file["/data/truth"]
        T = grid_file["/data/time"][:]
        IDX = grid_file["/data/idxs"][:]
        assert T.shape[0] == Y.shape[0]
        assert T.shape[0] == P.shape[0]
        assert IDX.shape[0] == Y.shape[1]
        assert IDX.shape[0] == P.shape[1]
        for i in range(P.shape[0]):
            yield (Y[i,...], P[i,...], IDX, T[i,1])

def parse_grid_params(grid_h5:Path):
    """
    Simple method to extract the parameter dict from a grid h5.

    Returns the original grid shape and the generators.gen_timegrid_subgrids
    arguments used to initialize the generator for the file as a 2-tuple:
    (grid_shape, gen_args)
    """
    with h5py.File(grid_h5, "r") as tmpf:
        gen_args = json.loads(tmpf["data"].attrs["gen_args"])
        grid_shape = tmpf["data"].attrs["grid_shape"]
        model_config = json.loads(tmpf["data"].attrs["model_config"])
        #grid_shape = tuple(int(v) for v in grid_shape if v.isnumeric())
    return (grid_shape, model_config, gen_args)

def bulk_grid_error_stats_to_hdf5(grid_h5:Path, stats_h5:Path,
        timesteps_chunk:int=32, debug=False):
    """
    Make a new hdf5 file given a gridded predictions hdf5 file, which contains
    error magnitude and bias statistics, collected across each pixel sequence

    Predictions and true values are stored as (T,P,Q,F) shaped arrays such that
    T : Timestep , P : Pixel (valid only) , Q : Stat quantity ,  F : pred feat

    Where the 'Q' axis has size 7 representing residual & state absolute error:
    (state_max, state_mean, state_stdev, state_final,
     res_max, res_mean, res_stdev)
    """
    gen = gen_grid_prediction_combos(grid_h5)
    grid_shape,model_config,gen_args = parse_grid_params(grid_h5)
    coarseness = gen_args.get("pred_coarseness", 1)
    F = None
    h5idx = 0
    for (ys,pr,ix,t) in gen:
        if debug:
            t = datetime.fromtimestamp(int(t))
            print(f"Loading timestep {t.strftime('%Y%m%d %H%M')}")
        if F is None:
            err_shape = (pr.shape[0], 7, pr.shape[-1])
            F = h5py.File(
                    name=stats_h5,
                    mode="w-",
                    ## use a 256MB cache (shouldn't matter)
                    rdcc_nbytes=256*1024**2,
                    )
            ## (N, P, Q, F) Predicted values
            S = F.create_dataset(
                    name="/data/stats",
                    shape=(0, *err_shape),
                    maxshape=(None, *err_shape),
                    chunks=(timesteps_chunk, *err_shape),
                    compression="gzip",
                    )
            ## (N,) Epoch times
            T = F.create_dataset(
                    name="/data/time",
                    shape=(0,),
                    maxshape=(None,),
                    compression="gzip",
                    )
            ## (P, 2) Valid pixel indeces
            IDX = F.create_dataset(
                    name="/data/idxs",
                    shape=ix.shape,
                    maxshape=ix.shape,
                    compression="gzip",
                    )
            IDX[...] = ix

            ## Load grid generator, original grid shape, and statistic labels
            ## as hdf5 attributes
            F["data"].attrs["gen_args"] = json.dumps(gen_args)
            F["data"].attrs["grid_shape"] = np.array(grid_shape)
            F["data"].attrs["model_config"] = json.dumps(model_config)
            F["data"].attrs["stat_labels"] = [
                    "state_error_max",
                    "state_error_mean",
                    "state_error_stdev",
                    "state_kge",
                    "state_pearson",
                    "state_bias_final",
                    "res_error_max",
                    "res_error_mean",
                    "res_error_stdev",
                    "res_kge",
                    "res_pearson",
                    ]

        ## subsample labels to the model's coarseness
        ys = ys[:,::coarseness,:]
        ## Predicted state
        ps = ys[:,0,:][:,np.newaxis,:] + np.cumsum(pr, axis=1)
        ## True residual
        yr = ys[:,1:] - ys[:,:-1]
        ## Bias in state
        bs = ps - ys[:,1:,:]
        ## Error in state
        es = np.abs(bs)
        ## Error in residual
        er = np.abs(pr - yr)

        ## Calculate state and residual kling-gupta efficiency
        pc_s = mm.pearson_coeff(ys[:,1:], ps)
        alpha_s = np.std(ps, axis=1, keepdims=True) \
                / np.std(ys[:,1:], axis=1, keepdims=True)
        beta_s = np.average(ps, axis=1, keepdims=True) \
                / np.average(ys[:,1:], axis=1, keepdims=True)
        kge_s = 1 - ((pc_s-1)**2 + (alpha_s-1)**2 + (beta_s-1)**2)**(1/2)
        pc_r = mm.pearson_coeff(yr, pr)
        alpha_r = np.std(pr, axis=1, keepdims=True) \
                / np.std(yr, axis=1, keepdims=True)
        beta_r = np.average(pr, axis=1, keepdims=True) \
                / np.average(yr, axis=1, keepdims=True)
        kge_r = 1 - ((pc_r-1)**2 + (alpha_r-1)**2 + (beta_r-1)**2)**(1/2)

        ## Stack to (P, Q, F) array
        stats = np.stack([
            np.amax(es, axis=1),
            np.average(es, axis=1),
            np.std(es, axis=1),
            np.squeeze(kge_s, axis=1),
            np.squeeze(pc_s, axis=1),
            bs[:,-1,:],
            np.amax(er, axis=1),
            np.average(er, axis=1),
            np.std(er, axis=1),
            np.squeeze(kge_r, axis=1),
            np.squeeze(pc_r, axis=1),
            ], axis=1)

        S.resize((h5idx+1, *err_shape))
        T.resize((h5idx+1,))
        S[h5idx,...] = stats
        T[h5idx] = t.strftime("%s")
        h5idx += 1
    return

def parse_bulk_grid_params(bulk_grid_path:Path):
    """
    Simple method to extract the parameter dict from a grid h5.

    Returns the original grid shape and the generators.gen_timegrid_subgrids
    arguments used to initialize the generator for the file as a 3-tuple:
    (grid_shape, gen_args, stat_labels)
    """
    with h5py.File(bulk_grid_path, "r") as tmpf:
        gen_args = json.loads(tmpf["data"].attrs["gen_args"])
        grid_shape = tmpf["data"].attrs["grid_shape"]
        #grid_shape = tuple(int(v) for v in grid_shape if v.isnumeric())
        stat_labels = tuple(tmpf["data"].attrs["stat_labels"])
        model_config = json.loads(tmpf["data"].attrs["model_config"])
    return (grid_shape, model_config, gen_args, stat_labels)


def gen_bulk_grid_stats(bulk_grid_path:Path, init_time=None, final_time=None,
        buf_size_mb=128):
    """
    Yields a bulk statistic grid's data by timestep as a 3-tuple like
    (stats, idxs, time) such that:

    stats := (P,7,F) 7 stats for the P valid pixels' F features
    idxs := (P,2) 2d integer indeces for each of the valid pixels
    time := float epoch time for the current sample's pivot

    :@param bulk_grid_path: Path to a hdf5 from bulk_grid_error_stats_to_hdf5
    :@param init_time: datetime of initial pivot to include in yielded results
    :@param final_time: datetime of last pivot to include in yielded results
    :@param buf_size_mb: Size of hdf5 chunk read buffer in MB
    """
    with h5py.File(
            bulk_grid_path,
            mode="r",
            rdcc_nbytes=buf_size_mb*1024**2,
            rdcc_nslots=buf_size_mb*16,
            ) as grid_file:
        S = grid_file["/data/stats"]
        T = grid_file["/data/time"][...]
        IDX = grid_file["/data/idxs"][...]

        ## restrict timestep indeces by applying optional bounds
        if not init_time is None:
            m_init = (T >= int(init_time.strftime("%s")))
        else:
            m_init = np.full(T.shape, True)
        if not final_time is None:
            m_final = (T < int(final_time.strftime("%s")))
        else:
            m_final = np.full(T.shape, True)

        ## yield timesteps in order, one at a time
        time_idxs = np.where(np.logical_and(m_init,m_final))
        for tidx in time_idxs[0]:
            yield (S[tidx,...], IDX, T[tidx])

def gen_sequence_prediction_combos(
        seq_h5:Path, pred_h5:Path, batch_size=64, pred_coarseness=1,
        gen_window=False, gen_horizon=False, gen_static=False, shuffle=False,
        seed=None, gen_static_int=False, gen_times=False, buf_size_mb=128):
    """
    Generator reading uniformly-ordered model sequence and prediction hdf5s,
    and yielding their content as a series of chunkedd tuples.

    Yields a 2-tuple with nested tuples following the format below. When a
    data type is requested not to be generated, the file will not be read for
    those data, and the corresponding tuple elements are set to None.

    (
        (window, horizon, static, static_int, (label_time, pred_time)),
        (label_state, pred_residual)
    )

    :@param seq_h5_path: Sequence hdf5 file created by make_sequence_hdf5
    :@param pred_h5: Prediction hdf5 file containing model outputs given
        inputs drawn from seq_h5_path, stored in the exact same order.
    :@param batch_size: Number of samples per yielded array
    :@param pred_coarseness: Frequency of predicted outputs. This only serves
        to subset the truth values to the appropriate steps.
    :@param gen_*: Where True, the corresponding data type is read and
        yielded chunk-wise ; otherwise, None is returned instead.
    :@param buf_size_mb: Buffer size to allocate to each file's chunks.
    """
    with (
            h5py.File(
                seq_h5,
                mode="r",
                rdcc_nbytes=buf_size_mb*1024**2,
                rdcc_nslots=buf_size_mb*15,
                ) as seq_file,
            h5py.File(
                pred_h5,
                mode="r",
                rdcc_nbytes=buf_size_mb*1024**2,
                rdcc_nslots=buf_size_mb*15,
                ) as pred_file
            ):
        seq_params = generators.parse_sequence_params(seq_h5)
        pred_params = generators.parse_prediction_params(pred_h5)

        w_idxs,w_derived,_ = generators._parse_feat_idxs(
                out_feats=pred_params["window_feats"],
                src_feats=seq_params["window_feats"],
                static_feats=seq_params["static_feats"],
                derived_feats=pred_params["derived_feats"],
                )
        h_idxs,h_derived,_ = generators._parse_feat_idxs(
                out_feats=pred_params["horizon_feats"],
                src_feats=seq_params["horizon_feats"],
                static_feats=seq_params["static_feats"],
                derived_feats=pred_params["derived_feats"],
                alt_feats=seq_params["pred_feats"],
                )
        p_idxs,p_derived,_ = generators._parse_feat_idxs(
                out_feats=pred_params["pred_feats"],
                src_feats=seq_params["pred_feats"],
                static_feats=seq_params["static_feats"],
                derived_feats=pred_params["derived_feats"],
                )
        s_idxs = tuple(seq_params["static_feats"].index(l)
                for l in pred_params["static_feats"])

        ## Establish a list of slices based on the batch size
        sample_count = seq_file["/data/pred"].shape[0]
        remainder = sample_count % batch_size

        slices = [
                slice(batch_size*i,batch_size*(i+1))
                for i in range(sample_count // batch_size)
                ]
        if remainder != 0:
            slices.append(slice(sample_count-remainder, sample_count))
        if shuffle:
            rng = np.random.default_rng(seed=seed)
            rng.shuffle(slices)
        ## Iterate over the slices and yield them one-by-one
        for tmp_slice in slices:
            ## Note: some loss of generality here!! This implicitly assumes
            ## that the first true state value is the one right before the
            ## initial horizon input timestep, included for the residual.
            ## See the documentation from sequence_dataset above.
            y_all = seq_file["/data/pred"][tmp_slice,...]
            s = seq_file["/data/static"][tmp_slice,...]

            y = y_all[:,::pred_coarseness]
            y = generators._calc_feat_array(
                    src_array=y,
                    static_array=s[:,np.newaxis],
                    stored_feat_idxs=p_idxs,
                    derived_data=p_derived,
                    )
            p = pred_file["/data/preds"][tmp_slice,...]

            if gen_window:
                w = seq_file["/data/window"][tmp_slice,...]
                w = generators._calc_feat_array(
                        src_array=w,
                        static_array=s,
                        stored_feat_idxs=p_idxs,
                        derived_data=w_derived,
                        )
            else:
                w = None
            if gen_horizon:
                h = seq_file["/data/horizon"][tmp_slice,...]
                h = generators._calc_feat_array(
                        src_array=h,
                        static_array=s,
                        stored_feat_idxs=h_idxs,
                        derived_data=h_derived,
                        alt_info=None,
                        alt_array=y_all,
                        alt_to_src_shape_slices=(slice(0,None),slice(1,None)),
                        )
            else:
                h = None
            if gen_static:
                s = s[...,s_idxs]
            else:
                s = None
            if gen_static_int:
                si = seq_file["/data/static_int"][tmp_slice,...]
            else:
                si = None
            if gen_times:
                ty = seq_file["/data/time"][tmp_slice,...]
                tp = pred_file["/data/time"][tmp_slice,...]
            else:
                ty,tp = None,None

            yield ((w,h,s,si,(ty,tp)), (y,p))

def add_norm_layers(md:tt.ModelDir, weights_file:str=None,
        dynamic_norm_coeffs:dict={}, static_norm_coeffs:dict={},
        save_new_model=False):
    """
    Wrap a model with layers linearly scaling the window, horizon, and static
    inputs before and predicted outputs after running the model such that

    x' = (x - b) / m    and    y' = y * m + b

    where (m,b) are parameters specified per-feature in the coefficient dicts

    :@param ModelDir: ModelDir object for a trained model.
    :@param weights_file: Optional weights file to load. if left to None,
        the '_final.weights.h5' model is used.
    :@param dynamic_norm_coeffs: Dictionary mapping feature names to
        normalization coefficients for time-varying data
    :@param static_norm_coeffs: Dictionary mapping feature names to
        normalization coefficients for static data.
    :@param save_new_model: If True, the new model weights including
        normalization are saved to a new file with name ending "_normed"
    """
    if not weights_file is None:
        weights_file = Path(weights_file).name

    model = md.load_weights(weights_path=weights_file)
    w_norm = np.array([
        tuple(dynamic_norm_coeffs[k])
        if k in dynamic_norm_coeffs.keys() else (0,1)
        for k in md.config["feats"]["window_feats"]
        ])[np.newaxis,np.newaxis,:]
    h_norm = np.array([
        tuple(dynamic_norm_coeffs[k])
        if k in dynamic_norm_coeffs.keys() else (0,1)
        for k in md.config["feats"]["horizon_feats"]
        ])[np.newaxis,np.newaxis,:]
    s_norm = np.array([
        tuple(static_norm_coeffs[k])
        if k in static_norm_coeffs.keys() else (0,1)
        for k in md.config["feats"]["static_feats"]
        ])[np.newaxis,:]
    p_norm = np.array([
        tuple(dynamic_norm_coeffs[k])
        if k in dynamic_norm_coeffs.keys() else (0,1)
        for k in md.config["feats"]["pred_feats"]
        ])[np.newaxis,:]

    w_in = tf.keras.Input(
            shape=(
                md.config["model"]["window_size"],
                md.config["model"]["num_window_feats"],),
            name="in_window")
    h_in = tf.keras.Input(
            shape=(
                md.config["model"]["horizon_size"],
                md.config["model"]["num_horizon_feats"],),
            name="in_horizon")
    s_in = tf.keras.Input(
            shape=(md.config["model"]["num_static_feats"],),
            name="in_static")
    si_in = tf.keras.Input(
            shape=(md.config["model"]["num_static_int_feats"],),
            name="in_static_int")

    out = model((
        (w_in-w_norm[...,0])/w_norm[...,1],
        (h_in-h_norm[...,0])/h_norm[...,1],
        (s_in-s_norm[...,0])/s_norm[...,1],
        si_in
        )) * p_norm[...,1] + p_norm[...,0]

    new_model = tf.keras.Model(inputs=(w_in,h_in,s_in,si_in), outputs=out)
    return new_model

if __name__=="__main__":
    pass
