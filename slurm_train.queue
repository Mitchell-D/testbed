#!/bin/csh
### SLURM batch script

### Email address
#SBATCH --mail-user=mtd0012@uah.edu


### TOTAL processors (number of tasks)
#SBATCH --ntasks 2

### total run time estimate (D-HH:MM)
#SBATCH -t 3-00:00

### memory (MB per CPU)
#SBATCH --mem-per-cpu=32G

### Mail to user on job done and fail
#SBATCH --mail-type=END,FAIL

### Partition (queue), select shared for GPU
### Optionally specify a GPU type: --gres=gpu:rtx5000:1 or --gres=gpu:a100:1
##SBATCH -p shared --gres=gpu:a100:1
##SBATCH -p shared --gres=gpu:rtx5000:1
#SBATCH -p shared --gres=gpu:1

#SBATCH -J rec-1_train
#SBATCH -o /rhome/mdodson/testbed/data/slurm/slurm_train_lstm-rec_1.out ## STDOUT
#SBATCH -e /rhome/mdodson/testbed/data/slurm/slurm_train_lstm-rec_1.err ## STDERR

## ------------------------------------------------------------------- ##

###SBATCH -J ff3_train
###SBATCH -o /rhome/mdodson/testbed/data/slurm/slurm_train_basic-dense_3.out ## STDOUT
###SBATCH -e /rhome/mdodson/testbed/data/slurm/slurm_train_basic-dense_3.err ## STDERR

## ------------------------------------------------------------------- ##

###SBATCH -J s2s-3_train
###SBATCH -o /rhome/mdodson/testbed/data/slurm/slurm_train_lstm-s2s_4.out ## STDOUT
###SBATCH -e /rhome/mdodson/testbed/data/slurm/slurm_train_lstm-s2s_4.err ## STDERR

module load cuda
$CUDA_PATH/samples/bin/x86_64/linux/release/deviceQuery

### Set dynamic link loader path variable to include CUDA and bins from mamba
setenv LD_LIBRARY_PATH /common/pkgs/cuda/cuda-11.4/lib64
setenv LD_LIBRARY_PATH ${LD_LIBRARY_PATH}:/common/pkgs/cuda/cuda-11.4/extras/CUPTI/lib64
setenv LD_LIBRARY_PATH ${LD_LIBRARY_PATH}:/rhome/mdodson/.micromamba/envs/learn/lib
echo $LD_LIBRARY_PATH

cd /rhome/mdodson/testbed

set runcmd = /rhome/mdodson/.micromamba/envs/learn/bin/python

##${runcmd} -u train_basic_dense.py
##${runcmd} -u train_lstm_s2s.py
${runcmd} -u train_lstm_rec.py
